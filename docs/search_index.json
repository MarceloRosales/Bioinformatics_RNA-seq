[["index.html", "Bioinformatics_RNA-seq Chapter 1 Introduction to RNA sequencing (RNA_seq) 1.1 Downloading and setting up conda environments. 1.2 Install Miniconda, or Anaconda. 1.3 Alignment Procedures and files.", " Bioinformatics_RNA-seq J. Marcelo Rosales R. Created: 2021-08-10; Update: 2021-09-27 Chapter 1 Introduction to RNA sequencing (RNA_seq) An over simplification of the Process. Holds 4 main sections. Sample and sample analysis. Samples are taken, prepared and send to a company for analysis. (Illumina !?). Raw data (FASTQ) - Expression quantification. Differential expression analysis. Pathway and Enrichment analysis. Each of these sections are subdivided in other steps. Download or acquire the files 2021/07/02 Copy Trial data from: MK/土橋 to folder HD-PCFSU3-A/Experiments Data/Genewiz : The Report file is here. The Summary file is here. Raw Data open &quot;Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/60-499583816_60-501009934/Result/03_GeneExpression&quot; Report file open &quot;/Volumes/HD-PCFSU3-A/Experiments%20Data/Genewiz/60-499583816_60-501009934/60-499583816_60-501009934_GENEWIZ_RNASeq_Report.html&quot; Summary report file open &quot;/Volumes/HD-PCFSU3-A/Experiments%20Data/Genewiz/60-499583816_60-501009934/60-499583816_60-501009934_GENEWIZ_QC_Report.html&quot; To open folder in Finder with r code, is similar to terminal commands.: open . # To open the current working folder within Finder open ~ # To open the Home folder open / # To open the Root directory open Desktop/ open Music/ open /Library/ open /path/to/Directory/ # If folder have spaces, replace with %20% or other similar, or use &quot;&quot;. open &quot;/path/to/Directory/&quot; You can also launch (and update) applications from the Terminal without using Finder. For example, to open Safari, type: (But seem like it only works in terminal and can not do it with r/rstudio) 1.1 Downloading and setting up conda environments. Benefit: You can install all your tools in “Conda” and export as yml and you can share that environment or create a new one base on it, and by doing so, you can have all those tools automatically installed. 1.2 Install Miniconda, or Anaconda. Choose Anaconda if you: - Are new to conda or Python - Like the convenience of having Python and over 1500 scientific packages automatically installed at once - Have the time and disk space (a few minutes and 3 GB), and/or - Don’t want to install each of the packages you want to use individually. Choose Miniconda if you: - Do not mind installing each of the packages you want to use individually. - Do not have time or disk space to install over 1500 packages at once, and/or - Just want fast access to Python and the conda commands, and wish to sort out the other programs later. Mind that Windows and Mac installation and procedures are different. Does Windows need Linux to be intalled? Yes, a version of Linux kernell for window is available for download. Install Anaconda or Miniconda. brew install --cask anaconda brew install --cask miniconda # To check the version of conda installed. conda --version Quick video tutorial on how to use conda. * (Master) Conda environment. * Set up a data Science environment with Conda-Forge. To create an environment. The easy way: Open the anaconda app &gt; Environment &gt; New &gt; Name… Ok To access environment and open in terminal: anaconda app &gt; Environment &gt; Select environment &gt; rna-seq_test ^ &gt; »Open in Terminal. 1.2.1 Conda terminal commands. Conda cheat sheet In Terminal # To open conda app open /Applications/Anaconda-Navigator.app # Or, if any of the container folders have a space character. open &quot;/Applications/Anaconda-Navigator.app&quot; # To open (activate) conda environment. . /usr/local/anaconda3/bin/activate &amp;&amp; conda activate /usr/local/anaconda3/envs/rna-seq_test # To see all the environment created. conda env list # To create a new environment called &quot;rna-seq_test&quot; in conda in terminal: conda create --rna-seq_test # To activate environment. conda activate rna-seq_test # To see if we activated the environment and where (path) we are. echo $path # To exit the environment and return to base/root. Check with echo $path. conda deactivate # To see packages installed in env.: conda config --show channels # List all packages and versions installed in active environment conda list # To see the url location of packages (repository): conda info # To add packages to the environment. (only to this environment: --env). conda config --env --add channels conda-forge conda config --env --add channels bioconda # If a package is repeated in the environment, Conda will always install the package of higher version. If you dont want that, and specifically need a version originally configured at creation of the environment.. type: conda config --env --set channel_priority strict. # To install fastqc conda install fastqc #To make and export a list of the required environment/ conda list --export &gt; requirements.txt #To make and export an yml file to share with others. conda env export &gt; rna-seq_test.yml conda env export --file rna-seq_test.yml # To quickly check the yml file. less rna-seq_test.yml #To see the content of a .yml file. head rna-seq_test.yml #To clear the terminal screen. clear # To run a Jupyter Notebook. jupyter notebook # To Keep Anaconda updated. conda update conda # Delete an environment and everything in it called rna-seq_test. conda env remove --rna-seq_test # To Uninstall Anaconda. # [Uninstall page]() 1.2.2 Anaconda/Miniconda install Summary of Steps. Install anaconda/miniconda for your OS from the website. Prevent the base environment from automatically activating. conda config --set auto_activate_base false Create an empty environment. conda create -n rna-seq_test. Activate the environment conda activate rna-seq_test. Add conda-forge as first channel conda config --env --add channels conda-forge. Ensure that conda-forge is used if the package is available. conda config --env --set channel_priority strict Install packages conda install .... Install packages not in conda-forge. Search in conda webpage search. conda install -c .... Verify Jupyter Notebooks in correct environment …!?… In jupyter: sys.executable Video reference: Bioinformatics - Downloading and Setting Up Conda Environments. 1.2.3 Conda activate error When trying to initialize environment in terminal with the conda activate rna-seq_testcommand, the following message pops up. CommandNotFoundError: Your shell has not been properly configured to use &#39;conda activate&#39;. To initialize your shell, run $ conda init &lt;SHELL_NAME&gt; Currently supported shells are: - bash - fish - tcsh - xonsh - zsh - powershell See &#39;conda init --help&#39; for more information and options. IMPORTANT: You may need to close and restart your shell after running &#39;conda init&#39;. I tried: conda init powershell conda init zsh So far the only way to enter environment is though anaconda app. open &quot;/Applications/Anaconda-Navigator.app&quot; Then Environment &gt; rna-seq_test» &gt; Open in terminal… [Terminal] Another temporary solution is using the code: # For the rna-seq_test environment. . /usr/local/anaconda3/bin/activate &amp;&amp; conda activate /usr/local/anaconda3/envs/rna-seq_test # For anaconda3 base environment. . /usr/local/anaconda3/bin/activate &amp;&amp; conda activate /usr/local/anaconda3; 1.3 Alignment Procedures and files. This section was created based in this reference. After installing “conda,” and to gain time, download the genome of the species to be study. In this case, download the mouse (mus muslculus) genome file. This file contains the list of all the mouse genes and their codes. We need something to map our reads against once we get to the results. Files for mouse (Mus musculus) are available for download at Johns Hopkins University Center for Computational Biology (CCB) at TopHat. A spliced read mapper for RNA-Seq. There are different ways: Using tophat Using Star Custom/personal code. 1.3.1 Using Tophat (early version) Not Working!!! To download: 1. Look for the Mus musculus Build37.2 from the NCBI. 1. Right click the link and copy the link address. 1. go back to the conda environment and type wget and the paste the link. wget ftp://igenome:G3nom3s4u@ussd-ftp.illumina.com/Mus_musculus/NCBI/build37.2/Mus_musculus_NCBI_build37.2.tar.gz 1.3.2 Using Tophat Not Working!!! Tophat can be installed using the same conda install conda install \\-c bioconda tophat When this is finished installing, then we will need to get the mouse genome from the Johns Hopkins Univeristy Center for computational BIology. The version of the mouse genome that I am using here is the NCBI build37.2. Instead of downloading this from the website and having to move it to the cluser, I will just download it using wget into the folder that has the raw reads, trimmed reads, and the FastQC files. wget ftp://igenome:G3nom3s4u@ussd-ftp.illumina.com/Mus_musculus/NCBI/build37.2/Mus_musculus_NCBI_build37.2.tar.gz This will take a long time to download because the file is a little less than 16GB zipped. Installation of TopHat failed.!!!!!!!!!!!!!!!!!!!!!!! Try STAR 1.3.3 Using STAR Complicated!!! STAR can be installed the same way as the previous programs with conda install (conda install -c bioconda star). In order to run STAR, we need to creaate indices just like with tophat, but STAR has this built in. I’m going to be using the same genome and GTF file as previously downloaded, but Dr. Ge uses a different zipped genome from the gencode database. ~/miniconda2/bin/STAR \\ --runThreadN 80 \\ --runMode genomeGenerate \\ --genomeDir starIndex \\ --genomeFastaFiles Index/genome.fa \\ #same when we made the bowtie indices --sjdbGTFfile Mus_musculus/NCBI/build37.2/Annotation/Archives/archive-2015-07-17-14-32-40/Genes/genes.gtf With the index files made, we can start aligning with STAR. It’s important here than we only pick the paired end reads and not use all of the reads. Tophat is able to use all 4 reads but STAR doesn’t allow that, so we need to make sure that we feed in the large files from trimming. ~/miniconda2/bin/STAR --runThreadN 80 --genomeDir starIndex --readFilesIn 770_fp.fq 770_rp.fq --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix 2121770 --outSAMtype BAM SortedByCoordinate [here]() Complicated.!!!!!!!!!!!!!!!!!!!!!!! 1.3.4 Custom/personal way to Download the Mus_musculus_NCBI_build37.2.tar.gz file. Esaest so far. Go to the iGenomes Ready-To-Use Reference Sequences and Annotations from illumina.. Copy the Mus musculus (Mouse) NCBI build37.2 link. Paste the link in a new tab/window on a web browser (chrome) and click enter. Popup window will ask where to save the file, choose folder and save. File is 23Gb so it will take about 3 to 4 hrs to download. 1.3.5 Get the Sample files. Samples files come as “x_1.fastq.gz” or “x_2.fastq.gz,” where x is the name of the file, 1 is for forward and 2 for reverse pair reads. example: we will place them in a folder (00_Rawdata) mPDL_RNA7D_Ko1_S1_L001_R1_001.fastq.gz mPDL_RNA7D_Ko1_S1_L001_R2_001.fastq.gz mPDL_RNA7D_Ko2_S1_L001_R1_001.fastq.gz mPDL_RNA7D_Ko2_S1_L001_R2_001.fastq.gz mPDL_RNA7D_Ko3_S1_L001_R1_001.fastq.gz mPDL_RNA7D_Ko3_S1_L001_R2_001.fastq.gz… etc…. Download multiqc conda install multiqc 1.3.6 Starting to analize fastq.gz files Initialize fastqc fastqc Make an output directory named rawFastQC mkdir rawFastQC Now we have to process all fastq files contained in the folder (00_Rawdata) and place the results in the directory we just made “rawFastQC.” fastqc rawFastQC/*.fastq.gz -o rawFastQC/ Once analysis is finished. Go to the results folder (rawFastQC) cd rawFastQC “fastqc” created .zip files and a .html file report for each sample. To see them individually may be difficult and take time. For a more convenient way to visualize results, merge all sample reports (html file) in one file. Create the report by reading all the files contained in the results folder (rawFastQC) and combine them in one multiqc_report.html file using “multiqc.” # go to the results folder where the html report files are. cd rawFastQC # Check if the files are there. ls # Create the multiqc report. Type (the dot is important!) multiqc . # check if file was created. ls 1.3.7 Trimming with Trimmomatic conda install trimmomatic # parallel my also be required for task piping. conda install parallel 24:40 Conda was used again to run Trimmomatic. This isn’t as easy as using the wildcard like with FastQC because each output has to be personalized for the read files that are input into Trimmomatic. Also, we have to make sure that the adapter sequences are in the same folder that we are running so we can refer to them easily when calling the Trimmomatic program. In this case, we are using the TruSeq3-PE-2.fa adapter sequences For example: ~/miniconda2/bin/trimmomatic PE SRR2121770_1.fastq.gz SRR2121770_2.fastq.gz 770_fp.fq.gz 770_fu.fq.gz 770_rp.fq.gz 770_ru.fq.gz ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2keepBothReads LEADING:3 TRALING:3 MINLEN:36 &amp; This would be repeated for each of the pairs (12 in total). We are trimming paired-end reads with the TruSeq3-PE-2 adapters. We are chopping off the first and last 3 bases and if we end up with a sequence less than 36 bases, we get rid of it. We want to make sure that there are enough bases in a read to work with. These parameters can be tweaked for possibly better end results with less being discarded. When Trimmomatic is finished running, it will out put the total number of reads, the total number from both the forward and reverse reads that are kept, the number of only forward reads kept, the number of only reverse reads kept, and the number of discarded reads. The trimmed reads can be analyzed again with FastQC to see how well the trimming worked to make the file better quality. After running FastQC on the trimmed files we see that the quality of those that were really bad quality were improved. There were a few different metrics throughout all of the files that bounced from a warning before the failing, or from passing before to a warning, and so forth, overall creating better quality read files. Read Check Undrstantig Trimmomatic tutorial for this section !!!!!! after trimmomatic, files will be created, .gz and .log files. Use multiqc to merge all logs in one report file. Then check the timmomatic survival Reads. If all samples are over 55M (million) reads then samples are valid for further analysis. Check the order again. Trimmomatic &gt; alligment -&gt; w/ TopHap or STAR 1.3.8 Using STAR to map genes Using the Mus_musculus files we will compare our samples and find where in the genome our reads are. &gt; Build Genome Indexing and useful STAR Flags (quick). See also: novocraft aling. *Opens only on safari. Analyzind RNA-Seq data using Python3 Snakemake. File Snake file. Look for more information with rRNA and Microbial Contamination. Bioinformatics - Contamination QC and FeatureCounts Quick Look at Counts and Setting Up R Project "],["file-download-or-sra-qc-and-trimming.html", "Chapter 2 File Download (or SRA), QC, and Trimming 2.1 Trimmomatic 2.2 Indexing with STAR 2.3 Microbial contamination 2.4 Feature counts. 2.5 Setting R environment.", " Chapter 2 File Download (or SRA), QC, and Trimming Bioinformatics - SRA Download, QC, and Trimming. Create environment (avoid spaces or special characters). Init environment conda install sra-tools This will allow us to pull directly from the sra page. (where the sample ex. Files) Go to page. Select only the files needed. Then in the Selected &gt; click on Accession List &gt; Save (this saves a .txt file with the list of names of the selected files). Drag onto the file structure, so its now in system ls to see the files (SRR_Acc_List.txt) Check ‘head SRR_Acc_List.txt’ command Also you can use the vi SRR_Acc_List.txt command to see the list in the file. Ex. ①. SRR2121770\\ ②. SRR2121771\\ ③. SRR2121774\\ ④. SRR2121778\\ ⑤. Etc. Now that we have the list of files, we have to individually download each one of them. One way to do that is with the fastq-dump --gzi --split-file SRR2121770 &amp; command. ①. This command will take the SRR2121770 file, and save it as a .gzi file (to save space) ②. it will split the file in forward (1) and Reverse (2), ③. the “&amp;” sign means that it will run this command in the background. This will take some time since the file is big. A way to see if the process is finished or not is with the command jobs, which gives a list of the jobs in process or nothing if there are not jobs running. Another way to do it is with the pipe command, so we don’t have to do this process for each file. Use the command cat SRR_Acc_List.txt | parallel fastq-dump --gzi --split-file {}. ①. This will take the SRR_Acc_List.txt and catalog the list. ②. Then it will take each of the element in the list &quot;{}&quot; slipt them, and save them as gzi. ③. In the rawReads folder???.(create rawReads folder). you will have a list of .fastq.gz_1 and .fastq.gz_2 for each element of the list. `…rawReads$ ls`. ④. Each time something is changed in the files, create a new folder and save there so the originals are not modified. Ex. Trimmed. Once all files are downloaded, install multiqc. conda install multiqc. (if not installed) Before running fastqc, create an output directory where to save the processed files. mkdir rawFastQC (make sure you are in the right directory before making this folder). Then run fastqc: fastqc -t 64 rawReads/*fastq.gz -o rawFastQC This means, process files with fastqc using 64 treads (-t 64) windows/linux virtual machine; by taking all fastqc.gz files in the folder rawReads; and save in the output directory (-o) called rawFastQC. This process will take a long time depending on the number and size of the files. Once finished cd rawFastQC and check the files ls Then inside the folder, run multiqc in multiqc .. Once this finished, inside the folder a new “multiqc_report.html” will be created. Open the files and compare. 22:29 Once this quality control is finished we can pass to the trimming of the samples. 2.1 Trimmomatic Trimmomatic web page. Quick start Paired End: # [Trimmomatic web page](http://www.usadellab.org/cms/?page=trimmomatic). java -jar trimmomatic-0.39.jar PE input_forward.fq.gz input_reverse.fq.gz output_forward_paired.fq.gz output_forward_unpaired.fq.gz output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 Trimming and Filtering full parameters description and example: # datacarpentry [Trimming and Filtering](https://datacarpentry.org/wrangling-genomics/03-trimming/index.html) trimmomatic PE -threads 4 SRR_1056_1.fastq SRR_1056_2.fastq \\ SRR_1056_1.trimmed.fastq SRR_1056_1un.trimmed.fastq \\ SRR_1056_2.trimmed.fastq SRR_1056_2un.trimmed.fastq \\ ILLUMINACLIP:SRR_adapters.fa SLIDINGWINDOW:4:20 Install trimmomatic, conda install trimmomatic (if not installed). conda install trimmomatic # parallel my also be required for task piping. conda install parallel If we are going to trim the files, we need to make a new folder, so exit rawFastQC folder. cd.. and mkdir trimmedReads, we are going to point trimmomatic the this folder. # exit rawFastQC folder (remember there is a space between cd and .) cd .. # Create new folder for trimmed files mkdir trimmedReads We are going to pull our true adapter sequencer to our working directory. Then it will be a lot easier to point trimmomatic to that vs having to type in the really long path. cp ~/miniconda3/envs/name_of_environment/share/trimmomatic-0.39-1/adapters/TrueSeq3-PE-2.fa . (the last point is important, it means that the command will the done in this folder). This command copy the file TruSeq3-PE-2.fa file to our current folder, why? so the process will be done directly in our folder, instead of the need to type the path to the folder (if not, in the code, instead of the point, type the path to the directory.) cp ~/miniconda3/envs/rna-seq_test/share/trimmomatic-0.39-1/adapters/TrueSeq3-PE-2.fa . This gives error. Sometimes, due to updates in conda and trimmomatic, folder names might change or there is a space in a folders name. Check if this is the case by open \"/usr/local/anaconda3/envs/rna-seq_test/share/\". Seems like the version of timmomatic is “0.39-2” and not \"0.39-1, and I installed anaconda3 and not miniconda3. Correct code and try again. Full path is: /usr/local/anaconda3/envs/rna-seq_test/share/trimmomatic-0.39-2/adapters/TruSeq3-PE-2.fa, then the code would be: `cp /usr/local/anaconda3/envs/rna-seq_test/share/trimmomatic-0.39-2/adapters/TruSeq3-PE-2.fa .` Install Parallel, Conda install paralell Create new directory for the trimmed files. mkdir trimmedReads. Now run the trimmomatic. For my samples it would be… # Try 01 FAILED trimmomatic PE -threads 5 rawReads/mPDL_RNA7D_Ko1_S1_L001_R1_001.fastq.gz rawReads/mPDL_RNA7D_Ko1_S1_L001_R2_001.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko1_S1_L001 ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2keepBothReads LEADING:3 TRALING:3 MINLEN:36 2&gt; trimmedReads/mPDL_RNA7D_Ko1_S1_L001trimming.log # Try 02 FAILED trimmomatic PE -threads 5 rawReads/mPDL_RNA7D_Ko1_S1_L001_R1_001_1.fastq.gz rawReads/mPDL_RNA7D_Ko1_S1_L001_R2_001_2.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko1_S1_L001 ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2keepBothReads LEADING:3 TRALING:3 MINLEN:36 2&gt; trimmedReads/mPDL_RNA7D_Ko1_S1_L001trimming.log # Try 03 SUCCESS!!!??? trimmomatic PE rawReads/mPDL_RNA7D_Ko1_S1_L001_R1_001_1.fastq.gz rawReads/mPDL_RNA7D_Ko1_S1_L001_R2_001_2.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko1_S1_L001 ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/mPDL_RNA7D_Ko1_S1_L001trimming.log # Try 04 (Datacarpentry) trimmomatic PE -threads 4 rawReads/mPDL_RNA7D_Ko1_S1_L001_R1_001_1.fastq.gz rawReads/mPDL_RNA7D_Ko1_S1_L001_R2_001_2.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko1_S1_L001_R1_001 ILLUMINACLIP:SRR_adapters.fa SLIDINGWINDOW:4:20 # datacarpentry [Trimming and Filtering](https://datacarpentry.org/wrangling-genomics/03-trimming/index.html) $ trimmomatic PE -threads 4 SRR_1056_1.fastq SRR_1056_2.fastq \\ SRR_1056_1.trimmed.fastq SRR_1056_1un.trimmed.fastq \\ SRR_1056_2.trimmed.fastq SRR_1056_2un.trimmed.fastq \\ ILLUMINACLIP:SRR_adapters.fa SLIDINGWINDOW:4:20 # Sample 1 Success!!!!???? error but TrimmomaticPE: Completed successfully message. # error in TruSeq3-PE.fa (No such file or directory)... Change to TruSeq3-PE-2.fa (not tested yet) trimmomatic PE rawReads/mPDL_RNA7D_Ko1_S1_L001_R1_001_1.fastq.gz rawReads/mPDL_RNA7D_Ko1_S1_L001_R2_001_2.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko1_S1_L001_R1_001 ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/mPDL_RNA7D_Ko1_S1_L001trimming.log # Sample 2 Success!!!!???? error but TrimmomaticPE: Completed successfully message. # error in TruSeq3-PE.fa (No such file or directory)... Change to TruSeq3-PE-2.fa (not tested yet) trimmomatic PE rawReads/mPDL_RNA7D_Ko2_S1_L001_R1_001_1.fastq.gz rawReads/mPDL_RNA7D_Ko2_S1_L001_R2_001_2.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko2_S1_L001_R2_001 ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/mPDL_RNA7D_Ko2_S1_L001trimming.log java -jar trimmomatic-0.39.jar PE input_forward.fq.gz input_reverse.fq.gz output_forward_paired.fq.gz output_forward_unpaired.fq.gz output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 The PE means “Pair end Read”, because we have both forward and reverse ends (1 and 2); If we would have only one end read (1 or 2) then it would be only single read, for single reads we type SE (instead of the PE). Timmomatic usually use up to 5 threads, thus we use “-threads 5”. (What if we don’t type the # of threads? would it use all threads? Why is this important?….) Then type the path to the folder and files containing the files to be trimmed. rawReads/mPDL_RNA7D_Ko1_S1_L001_R1_001.fastq.gz (forward), and type also the reverse “rawReads/mPDL_RNA7D_Ko1_S1_L001_R2_001.fastq.gz” The “-baseout trimmedReads/mPDL_RNA7D_Ko1_S1_L001” means that; the base file name of the output will all have the name \" trimmedReads/mPDL_RNA7D_Ko1_S1_L001\". It will place the file in the trimmedReads folder and all the file with start with the name mPDL_RNA7D_Ko1_S1_L001. Then we use the True sequence adapter (TruSeq3-PE-2.fa) that we moved to the working folder by typing “ILLUMINACLIP:TruSeq3-PE-2.fa:” (you can use the tap for auto completion coz it is in the same folder) Then we place the trimming parameters??… “2:30:10:2keepBothReads”. Add more info here….. how to understand the trimmomatic flags? Understanding Trimmomatic The “LEADING:3 TRALING:3”means that; if the bases have bad quality, then trimm the leading 3 and trailing 3 bases. more info, If the quality is of very good quality we can remove 10, but we dont want to remove/through away that much information from out reeds. Then, since our reads are really short and we are looking at their quality, we want to set a mean length. “MINLEN:36”. So if any or our 51 base pair reads have more than 15 removed (51-15= 36), we are just not going to use or keep them because we lost ~33% of the information of that read, so is not going to be a good (quality?) read. Then we want to keep the output. “2&gt; mPDL_RNA7D_Ko1_S1_L001trimming.log`”, the “2” is to keep output, and then feed that into “&gt;” the “trimmedReads folder, and make (from all the ourput) a nice report like for fastQC and colled it”mPDL_RNA7D_Ko1_S1_L001trimming.log\". From all the code typed, there will be 5 outputs to be loged, …. Since this takes really long time, and option will be to do a cat with the reads and do 4 jobs at a time. For my files..?? # Not working cat Filename.txt | parallel -j 4 &quot;trimmomatic PE -threads 5 rawReads/{}_R1_001.fastq.gz rawReads/{}_R2_001.fastq.gz -baseout trimmedReads/{} ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2keepBothReads LEADING:3 TRALING:3 MINLEN:36 2&gt; trimmedReads/{}trimming.log&quot; In the tutorial for auto loading. cat Filename_list.txt | parallel -j 4 &quot;trimmomatic PE -threads 5 rawReads/{}_1.fastq.gz rawReads/{}_2.fastq.gz -baseout trimmedReads/{} ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2keepBothReads LEADING:3 TRALING:3 MINLEN:36 2&gt; trimmedReads/{}trimming.log&quot; Control + Z to stop running and add bj 1 (in root?) to run in the background. type jobs (in terminal) to see the jobs running in the background. In the github tutorial: Conda was used again to run Trimmomatic. This isn’t as easy as using the wildcard like with FastQC because each output has to be personalized for the read files that are input into Trimmomatic. Also, we have to make sure that the adapter sequences are in the same folder that we are running so we can refer to them easily when calling the Trimmomatic program. In this case, we are using the TruSeq3-PE-2.fa adapter sequences For example: ~/miniconda2/bin/trimmomatic PE SRR2121770_1.fastq.gz SRR2121770_2.fastq.gz 770_fp.fq.gz 770_fu.fq.gz 770_rp.fq.gz 770_ru.fq.gz ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2keepBothReads LEADING:3 TRALING:3 MINLEN:36 &amp; Finally for this trial I used: # Sample 1 Success!!!!???? error but TrimmomaticPE: Completed successfully message. # error in TruSeq3-PE.fa (No such file or directory)... Change to TruSeq3-PE-2.fa (not tested yet) trimmomatic PE rawReads/mPDL_RNA7D_Ko1_S1_L001_R1_001_1.fastq.gz rawReads/mPDL_RNA7D_Ko1_S1_L001_R2_001_2.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko1_S1_L001_R1_001 ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/mPDL_RNA7D_Ko1_S1_L001trimming.log # Sample 2 Success!!!!???? error but TrimmomaticPE: Completed successfully message. # error in TruSeq3-PE.fa (No such file or directory)... Change to TruSeq3-PE-2.fa (not tested yet) trimmomatic PE rawReads/mPDL_RNA7D_Ko2_S1_L001_R1_001_1.fastq.gz rawReads/mPDL_RNA7D_Ko2_S1_L001_R2_001_2.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko2_S1_L001_R2_001 ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/mPDL_RNA7D_Ko2_S1_L001trimming.log Pipe line code for automatic file loading to trimmomatic still not successfully tested. Come back to this later. # Pipe and loop for all files. cat filenames.txt | parallel -j 4 &quot;trimmomatic PE rawReads/{}_1.fastq.gz rawReads/{}_2.fastq.gz -baseout trimmedReads/{} ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/{}trimming.log&quot; #Not successful. Finally re-run the fastqc and multiqc of the trimmed data. # In TrimmedRead folder ls multiqc . This will take all the .log files created in by the TruSeq3-PE-2.fa trimmed data. Note: Seems like multiqc can take all html and log files and combine the information with in them in a single html report file. # fastQC of trimmed files again? mkdir trimmedFastQC # rm -d trimmedFastQC # Make sure folder is in main folder, if not delete # Run fastqc... Trimmed files have no file extension. Why??? fastqc trimmedReads/* -o trimmedFastQC # .zip and qc html files will be created. Run multiqc cd trimmedFastQC multiqc . 2.2 Indexing with STAR Bioinformatics - Building Genome Index and Aligning with STAR STAR is a tool that allow us to build an index of the mouse genome; then, with those index files we can then map the reads against the genome and find the location of each of those reads where they are mapped in the genome. Once we have that, the we can do a the “read count” For more information see [STAR: ultrafast universal RNA-seq aligner] (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3530905/) STAR Manual download link. PDF file in local HD Link to manual page. Download the Mouse genome form the NCBI page. See Custom/personal way to Download the Mus_musculus_NCBI_build37.2.tar.gz file section. (open section in a new tab) See 1.3.4 Once file is downloaded, unzip. (It will take a long time) Make sure to be in the same folder/directory as the tar.gz file. extracted files will be saved in a Mus_musculus folder/directory automatically created. # Check the correct environment. . /usr/local/anaconda3/bin/activate &amp;&amp; conda activate /usr/local/anaconda3/envs/rna-seq_test # Check the correct folder/directory. cd &quot;/Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/60-499583816_60-501009934/Result/Mus Musculus/Mus_musculus_NCBI_build37.2.tar.gz&quot; # cd &quot;folder path&quot; #Data/Genewiz/60-499583816_60-501009934/Result/Mus Musculus/Mus_musculus_NCBI_build37.2.tar.gz: Not a directory cd &quot;/Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/60-499583816_60-501009934/Result/Mus Musculus/&quot; # Check contents ls # Unzip file tar xvfz Mus_musculus_NCBI_build37.2.tar.gz Tar command info for linux here and for mac here Confirm that STAR is installed if not Install STAR. # Make sure of enviroment. STAR --help #If not found install. conda install STAR # Make sure is from &quot;bioconda&quot;. Preceed([y]/n)? message. y # Check again. Long list of flags. STAR --help # Create a directory for starIndex. mkdir starIndex ls Run STAR. Make sure that the unzip folder of Mus_musculus_NCBI_build37.2.tar.gz (Mus_musculus) is in the same directory as trimmed/environment. Is this necessary???! FYI: Mus_musculus_NCBI_build37.2.tar.gz zip file is 23.58 GB. Mus_musculus unziped folder is 98.77 GB. Initially downloaded into: \"/Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/60-499583816_60-501009934/Result/Mus Musculus/Mus_musculus_NCBI_build37.2.tar.gz\" Trial analysis forlder in: \"/Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/Trial02\" Mus_musculus unziped folder moved to: /Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/Trial02/Mus_musculus Path to genome.fa file. /Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/60-499583816_60-501009934/Result/Mus Musculus/Mus_musculus/NCBI/build37.2/Sequence/WholeGenomeFasta/genome.fa Path to genes.gtf file. /Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/60-499583816_60-501009934/Result/Mus Musculus/Mus_musculus/NCBI/build37.2/Annotation/Archives/archive-2015-07-17-14-32-40/Genes/genes.gtf However better to use the folder Shortcut (known as aliases on Mac) created when unziped. This eliminates the Archives/archive-2015-07-17-14-32-40section of the path. So… use /Mus_musculus/NCBI/build37.2/Annotation/Genes/genes.gtf # Make sure that the unzip folder of Mus_musculus_NCBI_build37.2.tar.gz (Mus_musculus) is in the same directory as environment/analysis folder in this case: cd &quot;/Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/Trial02/&quot; # Run STAR STAR --runThreadN 64 --runMode genomeGenerate --genomeDir starIndex --genomeFastaFiles Mus_musculus/NCBI/build37.2/Sequence/WholeGenomeFasta/genome.fa --sjdbGTFfile Mus_musculus/NCBI/build37.2/Annotation/Genes/genes.gtf Sep 13 12:47:47 ..... started STAR run Sep 13 12:47:48 ... starting to generate Genome files Sep 13 12:49:24 ..... processing annotations GTF Sep 13 12:49:40 ... starting to sort Suffix Array. This may take a long time... Sep 13 12:49:51 ... sorting Suffix Array chunks and saving them to disk... Sep 13 13:23:01 ... loading chunks from disk, packing SA... Sep 13 13:48:03 ... finished generating suffix array Sep 13 13:48:03 ... generating Suffix Array index Sep 13 13:53:59 ... completed Suffix Array index Sep 13 13:54:00 ..... inserting junctions into the genome indices # Try several times but it could not finished correctly, &quot;System run out of application memory. # It is important that the hole process is fully completed. It takes quite a long time and uses a lot of memory resource. Your System Has Run Out of Application Memory on Mac. Solutions: 1. Close unnecessary webpages/programs. 2. Free up more space on the system HDD (at least 20% of free memory?). 3. Google chrome application memory leak?. Run task manager and check memory usage. 4. Check memory pressure (memory pressure command) and double ckeck your free space. Also try running sudo purge from the terminal. 5. Re-start computer to reset uptime. 6. Update software. 7. Reset Mac’s NVRAM and PRAM. 8. Close chrome and/or Safari.# STAR version: 2.7.9a compiled: :/Users/cshl/data/STAR/STAR/source Sep 13 16:19:40 ..... started STAR run Sep 13 16:19:41 ... starting to generate Genome files Sep 13 16:21:14 ..... processing annotations GTF Sep 13 16:21:28 ... starting to sort Suffix Array. This may take a long time... Sep 13 16:21:37 ... sorting Suffix Array chunks and saving them to disk... Sep 13 16:53:49 ... loading chunks from disk, packing SA... Sep 13 17:18:12 ... finished generating suffix array Sep 13 17:18:12 ... generating Suffix Array index Sep 13 17:23:21 ... completed Suffix Array index Sep 13 17:23:24 ..... inserting junctions into the genome indices Sep 13 17:25:56 ... writing Genome to disk ... Sep 13 17:27:06 ... writing Suffix Array to disk ... Sep 13 17:36:19 ... writing SAindex to disk Sep 13 17:37:01 ..... finished successfully (rna-seq_test) MR-MBP20:Trial02 marcelorosales$ # Check if the files were created in the folder. cd starIndex ls These process will take about 2 hrs or longer depending con computer’s processing power. The optins/flags coded were: –runThreadN 64: Number of threads use for processing. –runMode genomeGenerate: To start star generates the index files therefore genomeGenerate. –genomeDir starIndex:This is the directory for the star index output (is not the genome FASTA files). –genomeFastaFiles Mus_musculus/NCBI/build37.2/Sequence/WholeGenomeFasta/genome.fa: the whole genome of the mouse is in this file. –sjdbGTFfile Mus_musculus/NCBI/build37.2/Annotation/Genes/genes.gtf: This lets star know that the next file we are going to give it is a GTF file which is the anotation file of all the transcript that we are going to try to identify with our reads. It may not be really necessary, but it is recommended to usefor more accuracy, specially in de novo transcrip discovery, you would use this basically to take all the reads and map them. STAR will create the index files and save them in the “starIndex” folder. Whit this files compleated, we will now map the reads of our samples Align reads Create a Folder to save the sample reads. # create a new directory to target as output directory. mkdir starAligned ls For aligning, there are some issues with the flags command Z cat which tells star that the files we’re inputting are zipped. The way around it is to (maybe not the best way) gunzip flag (command/app). The way to use this is to do/perform the command beforehand, and then input on the files that we want, which it would be something like treammedReads/[] in a cat file. (file containing a catalog or list of the files downloaded or to be fed to the application). To create a cat file see Cat command in Linux or see video cat &gt; filenames.txt # Type or copy paste. mPDL_RNA7D_Ko1_S1_L001_R1_001_1P mPDL_RNA7D_Ko1_S1_L001_R1_001_2P mPDL_RNA7D_Ko2_S1_L001_R2_001_1P mPDL_RNA7D_Ko2_S1_L001_R2_001_2P # press control +D to save # Also, might need to instal pigz conda install -c conda-forge pigz mPDL_RNA7D_Ko1_S1_L001_R1_001_1P mPDL_RNA7D_Ko1_S1_L001_R1_001_1U mPDL_RNA7D_Ko1_S1_L001_R1_001_2P mPDL_RNA7D_Ko1_S1_L001_R1_001_2U mPDL_RNA7D_Ko2_S1_L001_R2_001_1P mPDL_RNA7D_Ko2_S1_L001_R2_001_1U mPDL_RNA7D_Ko2_S1_L001_R2_001_2P mPDL_RNA7D_Ko2_S1_L001_R2_001_2U mPDL_RNA7D_Ko1_S1_L001_R1_001_1P mPDL_RNA7D_Ko1_S1_L001_R1_001_2P mPDL_RNA7D_Ko2_S1_L001_R2_001_1P mPDL_RNA7D_Ko2_S1_L001_R2_001_2P changed to match filenames.txt mPDL_RNA7D_Ko1_1P mPDL_RNA7D_Ko1_2P mPDL_RNA7D_Ko2_1P mPDL_RNA7D_Ko2_2P # Original cat filenames2.txt | parallel -j 2 &quot;gunzip trimmedReads/{}_*P.gz &amp;&amp; STAR --runThreadN 32 --genomeDir starIndex --readFilesIn trimmedReads/{}_1P trimmedReads/{}_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/{} --outSAMtype BAM SortedByCoordinate &amp;&amp; pigz trimmedReads/{}_*P&quot; # ERROR # zsh:1: no matches found: trimmedReads/mPDL_RNA7D_Ko1_*P.gz # zsh:1: no matches found: trimmedReads/mPDL_RNA7D_Ko2_*P.gz # zsh:1: no matches found: trimmedReads/mPDL_RNA7D_Ko1_*P.gz # zsh:1: no matches found: trimmedReads/mPDL_RNA7D_Ko2_*P.gz # With no extensions. cat filenames2.txt | parallel -j 2 &quot;gunzip trimmedReads/{}_*P &amp;&amp; STAR --runThreadN 32 --genomeDir starIndex --readFilesIn trimmedReads/{}_1P trimmedReads/{}_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/{} --outSAMtype BAM SortedByCoordinate &amp;&amp; pigz trimmedReads/{}_*P&quot; # ERROR #gunzip: trimmedReads/mPDL_RNA7D_Ko1_1P: unknown suffix -- ignored #gunzip: trimmedReads/mPDL_RNA7D_Ko1_2P: unknown suffix -- ignored #gunzip: trimmedReads/mPDL_RNA7D_Ko2_1P: unknown suffix -- ignored #gunzip: trimmedReads/mPDL_RNA7D_Ko2_2P: unknown suffix -- ignored cat filenames2.txt | parallel -j 2 &quot;STAR --runThreadN 32 --genomeDir starIndex --readFilesIn trimmedReads/{}_1P trimmedReads/{}_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/{} --outSAMtype BAM SortedByCoordinate &amp;&amp; pigz trimmedReads/{}_*P&quot; # cat filenames2.txt | parallel -j 2 &quot;gunzip trimmedReads/{}_*P.gz &amp;&amp; STAR --runThreadN 32 --genomeDir starIndex --readFilesIn trimmedReads/{}_1P trimmedReads/{}_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/{} --outSAMtype BAM SortedByCoordinate &amp;&amp; pigz trimmedReads/{}_*P&quot; # No gunzip cat filenames2.txt | parallel -j 2 &quot;STAR --runThreadN 32 --genomeDir starIndex --readFilesIn trimmedReads/{}_1P trimmedReads/{}_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/{} --outSAMtype BAM SortedByCoordinate &amp;&amp; pigz trimmedReads/{}_*P&quot; ## NOT WORKING... OPTIONS: # 1. FIRST convert all .*P files to gz () and do it in the background (use &amp; at the end of line). pigz trimmedReads/*P &amp; # Check bg 1 # to see if it is working jobs # [1] running pigz trimmedReads/*P # to see Process type top # see commad &quot;pigz&quot; &quot;STAR&quot; if running. also top # To stop control + Z # To exit control + C # Now all P files are gz. run code. cat filenames2.txt | parallel -j 2 &quot;gunzip trimmedReads/{}_*P.gz &amp;&amp; STAR --runThreadN 32 --genomeDir starIndex --readFilesIn trimmedReads/{}_1P trimmedReads/{}_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/{} --outSAMtype BAM SortedByCoordinate &amp;&amp; pigz trimmedReads/{}_*P&quot; &amp; # Found better code. cat filenames2.txt | parallel -j 2 &quot;parallel gunzip ::: trimmedReads/{}_*P.gz &amp;&amp; STAR --runThreadN 64 --genomeLoad LoadAndKeep --genomeDir starIndex/ --readFilesIn trimmedReads/{}_1P trimmedReads/{}_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/{} --limitBAMsortRAM 5000000000 --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx &amp;&amp; pigz trimmedReads/{}_*P&quot; &amp; # I there is no need to unzip cat filenames2.txt | parallel -j 2 &quot;STAR --runThreadN 64 --genomeLoad LoadAndKeep --genomeDir starIndex/ --readFilesIn trimmedReads/{}_1P trimmedReads/{}_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/{} --limitBAMsortRAM 5000000000 --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx &amp;&amp; pigz trimmedReads/{}_*P&quot; # ERROR # Shared memory error: 4, errno: Invalid argument(22) # EXITING because of FATAL ERROR: problems with shared memory: error from shmget() or shm_open(). # SOLUTION: check shared memory settings as explained in STAR manual, OR run STAR with --genomeLoad NoSharedMemory to avoid using shared memory cat filenames2.txt | parallel -j 2 &quot;STAR --runThreadN 64 --genomeLoad NoSharedMemory --genomeDir starIndex/ --readFilesIn trimmedReads/{}_1P trimmedReads/{}_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/{} --limitBAMsortRAM 5000000000 --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx&quot; # ERROR # BAMoutput.cpp:27:BAMoutput: exiting because of *OUTPUT FILE* error: could not create output file starAligned/mPDL_RNA7D_Ko2_STARtmp//BAMsort/49/2 # SOLUTION: check that the path exists and you have write permission for this file. Also check ulimit -n and increase it to allow more open files. ulimit -n #2560 ulimit -n 100000 fg1 #? What us tgus for? cat filenames2.txt | parallel -j 2 &quot;STAR --runThreadN 64 --genomeLoad NoSharedMemory --genomeDir starIndex/ --readFilesIn trimmedReads/{}_1P trimmedReads/{}_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/{} --limitBAMsortRAM 5000000000 --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx&quot; cat filenames2.txt | parallel -j 1 &quot;STAR --runThreadN 64 --genomeLoad NoSharedMemory --genomeDir starIndex/ --readFilesIn trimmedReads/{}_1P trimmedReads/{}_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/{} --limitBAMsortRAM 5000000000 --outSAMtype BAM SortedByCoordinate --outReadsUnmapped Fastx&quot; STAR --runThreadN 32 --genomeDir starIndex --readFilesIn trimmedReads/mPDL_RNA7D_Ko1_S1_L001_R1_001_1P trimmedReads/mPDL_RNA7D_Ko1_S1_L001_R1_001_2P --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix starAligned/mPDL_RNA7D_Ko1_S1_L001_R1_001 --outSAMtype BAM SortedByCoordinate SAM is human readable, BAM is compress binary version (for CPU to read), 2.3 Microbial contamination For this use/install Kraken2 2.4 Feature counts. video # Install subreads. conda install subread # count featureCounts -h cd &quot;/Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/Trial02/Mus_musculus/NCBI/build37.2/Annotation/Archives/archive-2015-07-17-14-32-40/Genes/&quot; head genes.gtf # GTF file has gene_id (sametimes is doesnt work!), and transcipt_id. This two are going to be used to label genes. in the fearuresCounts -h you can see the code to input the *_id. # make a count Directoru cd &quot;/Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/Trial02&quot; mkdir readCounts # Read counts # as .txt featureCounts -T 8 -a Mus_musculus/NCBI/build37.2/Annotation/Archives/archive-2015-07-17-14-32-40/Genes/genes.gtf -g `transcript_id` -o readCounts/readCounts.txt starAlign/*.bam # as .csv? featureCounts -T 8 -a Mus_musculus/NCBI/build37.2/Annotation/Archives/archive-2015-07-17-14-32-40/Genes/genes.gtf -g `transcript_id` -o readCounts/readCounts.csv starAlign/*.bam # make the report cd readCounts multiqc . 2.5 Setting R environment. Intall packages Packages readr (install.packages(‘readr’)) limma (BiocManager::install(‘limma’)) DESeq2 (BiocManager::install(‘DESeq2’)) dplyr (install.packages(“dplyr”)) ggplot2 (install.packages(“ggplot2”)) gplots (install.packages(“gplots”)) Annotations (BiocManager::install(‘AnnotationDbi’)) org.Hs.eg.db (BiocManager::install(‘org.Hs.eg.db’)) This is for Human org.Mm.eg.db (BiocManager::install(‘org.Mm.eg.db’)) This is for Mouse ggrepel (install.packages(“ggrepel”)) ReportingTools (BiocManager::install(‘ReportingTools’)) GO.db (BiocManager::install(‘GO.db’)) GOstats (BiocManager::install(‘GOstats’)) pathview (BiocManager::install(‘pathview’)) gage (BiocManager::install(‘gage’)) gageData (BiocManager::install(‘gageData’)) select (BiocManager::install(‘Select’)) install.packages(c(&quot;dplyr&quot;, &quot;ggplots&quot;, &quot;ggplot2&quot;, &quot;greppel&quot; )) BiocManager::install(c(&quot;lima&quot;, &quot;DESeq2&quot;, &quot;AnotationDbi&quot;, &quot;org.Mn.eg.db&quot;, &quot;ReportingTools&quot;, &quot;GO.db&quot;, &quot;GOstats&quot;, &quot;pathview&quot;, &quot;gage&quot;, &quot;gageDATA&quot;, &quot;select&quot;)) # Libraries library(limma) library(DESeq2) library(dplyr) library(readr) countData = read_csv(&quot;readCounts.csv&quot;, skip = 1) ##End "],["methods.html", "Chapter 3 Methods", " Chapter 3 Methods We describe our methods in this chapter. Set Environment QC/multiqc of raw Reads Trimming with Trimmomatic QC/multiqc of trimmed Reads Genome Index and Aligning with STAR Raw Data file names original: mPDL_RNA7D_Ko2_S1_L001_R2_001_2.fastq.gz mPDL_RNA7D_Ko1_S1_L001_R1_001_1.fastq.gz mPDL_RNA7D_Ko1_S1_L001_R2_001_2.fastq.gz mPDL_RNA7D_Ko2_S1_L001_R1_001_1.fastq.gz Change (for easy management) to: mPDL_RNA7D_Ko1_1.fastq.gz mPDL_RNA7D_Ko1_2.fastq.gz mPDL_RNA7D_Ko2_1.fastq.gz mPDL_RNA7D_Ko2_2.fastq.gz Seems like original raw data was already splited # Create the cat file with the file names. cat &gt; filenames.txt #Type or copy paste the name of the files to be analyzed. # In last line press enter and then control + D to save the file names in the cat file. #Check the list head filenames.txt # Since already spliced not need to run this section. # In conda environment mkdir rawReads cd rawReads # Download files from webpage source. (one by one) fastq-dump --gzip --split-file mPDL_RNA7D_Ko1 &amp; # The &amp; indicates to run in the background. To see the jobs in the bkg. jobs # Download files from webpage source. (all at once). Better to do overnight. cat filenames.txt | parallel &quot;fastq-dump --gzip --split-file {}&quot; # files will be Forward (1) Reverse (2) or Paired reads. conda install multiqc # Some git preferences for git contributions. git status git config --list # set git email git config user.email &quot;email@example.com&quot; # confirm emal git config --global user.email # For the rna-seq_test environment. . /usr/local/anaconda3/bin/activate &amp;&amp; conda activate /usr/local/anaconda3/envs/rna-seq_test cd &quot;/Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/Trial02&quot; ## Run FAST QC mkdir rawFastQC fastqc -t 64 rawReads/*fastq.gz -o rawFastQC fastqc rawReads/*fastq.gz -o rawFastQC cd rawFastQC ls ## Run multyQC multiqc . ls cd.. ## Trimming cat &gt; filenames2.txt mPDL_RNA7D_Ko1 mPDL_RNA7D_Ko2 cat filenames2.txt mkdir trimmedReads # Pull TrueSeq to folder. cp /usr/local/anaconda3/envs/rna-seq_test/share/trimmomatic-0.39-2/adapters/TruSeq3-PE-2.fa . # Run Trimmomatic # Original (one by one) trimmomatic PE -threads 5 rawReads/mPDL_RNA7D_Ko1_1.fastq.gz rawReads/mPDL_RNA7D_Ko1_2.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko1 ILLUMINACLIP:truSeq-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/mPDL_RNA7D_Ko1trimming.log #???????????? HOw to determing leading, trailing and minimun length # Original (all automatically) cat filenames2.txt | parallel -j 4 &quot;trimmomatic PE -threads 5 rawReads/{}_1.fastq.gz rawReads/{}_2.fastq.gz -baseout trimmedReads/{} ILLUMINACLIP:truSeq-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/{}trimming.log&quot; # Check if jobs are running jobs # Problems: Takes a long time and sometimes it stops. # The generic trimmomatic command: java -jar trimmomatic-0.39.jar PE inputforward.fq.gz inputreverse.fq.gz outputforwardpaired.fq.gz outputforwardunpaired.fq.gz outputreversepaired.fq.gz outputreverseunpaired.fq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 # Try 03 SUCCESS!!!? trimmomatic PE rawReads/mPDL_RNA7D_Ko1_S1_L001_R1_001_1.fastq.gz rawReads/mPDL_RNA7D_Ko1_S1_L001_R2_001_2.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko1_S1_L001 ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/mPDL_RNA7D_Ko1_S1_L001trimming.log # Sample 1 Success!!!!???? error but TrimmomaticPE: Completed successfully message. # error in TruSeq3-PE.fa (No such file or directory)... Change to TruSeq3-PE-2.fa (not tested yet) trimmomatic PE rawReads/mPDL_RNA7D_Ko1_S1_L001_R1_001_1.fastq.gz rawReads/mPDL_RNA7D_Ko1_S1_L001_R2_001_2.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko1_S1_L001_R1_001 ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/mPDL_RNA7D_Ko1_S1_L001trimming.log # Sample 2 Success!!!!???? error but TrimmomaticPE: Completed successfully message. # error in TruSeq3-PE.fa (No such file or directory)... Change to TruSeq3-PE-2.fa (not tested yet) trimmomatic PE rawReads/mPDL_RNA7D_Ko2_S1_L001_R1_001_1.fastq.gz rawReads/mPDL_RNA7D_Ko2_S1_L001_R2_001_2.fastq.gz -baseout trimmedReads/mPDL_RNA7D_Ko2_S1_L001_R2_001 ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/mPDL_RNA7D_Ko2_S1_L001trimming.log # Pipe and loop for all files. cat filenames.txt | parallel -j 4 &quot;trimmomatic PE rawReads/{}_1.fastq.gz rawReads/{}_2.fastq.gz -baseout trimmedReads/{} ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2keepBothReads LEADING:3 TRAILING:3 MINLEN:36 2&gt; trimmedReads/{}trimming.log&quot; #Not successful. # In TrimmedRead run multiqc ls multiqc . # fastQC of trimmed files again? mkdir trimmedFastQC # rm -d trimmedFastQC # Make sure folder is in main folder, if not delete # Run fastqc... Trimmed files have no file extension. Why??? fastqc trimmedReads/* -o trimmedFastQC # .zip and qc html files will be created. Run multiqc cd trimmedFastQC multiqc . Alignment with STAR Download the Mouse genome form the NCBI page. See 1.3.4 Unzip the tar.gz file. FYI: This processes require a long time. # Check the correct environment. . /usr/local/anaconda3/bin/activate &amp;&amp; conda activate /usr/local/anaconda3/envs/rna-seq_test # Check the correct folder/directory. cd &quot;/Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/60-499583816_60-501009934/Result/Mus Musculus/Mus_musculus_NCBI_build37.2.tar.gz&quot; #Data/Genewiz/60-499583816_60-501009934/Result/Mus Musculus/Mus_musculus_NCBI_build37.2.tar.gz: Not a directory cd &quot;/Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/60-499583816_60-501009934/Result/Mus Musculus/&quot; # Unzip file tar xvfz Mus_musculus_NCBI_build37.2.tar.gz Confirm that STAR is installed if not Install STAR. # Make sure of enviroment. STAR --help #If not found install. conda install STAR # Make sure is from &quot;bioconda&quot;. Preceed([y]/n)? message. y # Check again. Long list of flags. STAR --help # Create directory for starIndex. mkdir starIndex Run STAR 1st. Index Mus_Musculus whole genome, 2nd. Index Samples against Mus_Musculus Index. # Make sure that the unzip folder of Mus_musculus_NCBI_build37.2.tar.gz (Mus_musculus) is in the same directory as environment/analysis folder in this case: cd &quot;/Volumes/HD-PCFSU3-A/Experiments Data/Genewiz/Trial02/&quot; # Run STAR. Takes long time. wait until ..... finished successfully message appears. if not rerun again. STAR --runThreadN 64 --runMode genomeGenerate --genomeDir starIndex --genomeFastaFiles Mus_musculus/NCBI/build37.2/Sequence/WholeGenomeFasta/genome.fa --sjdbGTFfile Mus_musculus/NCBI/build37.2/Annotation/Genes/genes.gtf Align reads # create a new directory mkdir starAligned Git token "],["external-ref.html", "Chapter 4 External Ref", " Chapter 4 External Ref We have finished a nice book. "],["rna-seq-differentially-expressed-go-enrichment-and-pathway-analysis.html", "Chapter 5 RNA Seq Differentially Expressed, GO Enrichment, and Pathway analysis 5.1 Background 5.2 Cleaning Data 5.3 Differentially Expressed Sequence Identification 5.4 Interactions cause a difference between the lfc betwen pooled data, e.g. p53+/+ (control and IR) and p53-/- (control and IR) 5.5 7. GO Enrichment analysis using GOstats 5.6 8. Pathway analysis using expression data", " Chapter 5 RNA Seq Differentially Expressed, GO Enrichment, and Pathway analysis based on: for STAT736 Alex Soupir Tutorial ACSoupir/Bioinformatics_RNASeq/Mouse_RNA_Seq_p53_genotoxic.Rmd (Raw md file.) See here for full view: Mouse_RNA_Seq_p53_genotoxic.md. 5.1 Background For STAT736-Fall-2019, we are analyzing the RNA-Seq from the publication Genome-wide analysis of p53 transcriptional programs in B cells upon exposure to genotoxic stress in vivo. We are only using the sequences B cells from spleen and not the non-B cells from spleen from the SRA Run Selector on NCBI. The mice were exposed to whole-body ionizing radiation and sequences were extracted from both Bcells and non-B cells from the spleens of the mice. Two genotypes of mice were used: mice with p53 knocked out and the wild-type C57/Bl6. There were 4 different group combinations including the 2 different genotypes; each genotype was subjected to the ionizing radiation as well as control/mock. library(knitr) experimental_design = data.frame(&#39;Genotype&#39; = c(&#39;p53&#39;, &#39;C57/Bl6&#39;, &#39;p53&#39;, &#39;C57/Bl6&#39;), &#39;Treatment&#39; = c(&#39;Mock&#39;, &#39;Mock&#39;, &#39;IR&#39;, &#39;IR&#39;)) rownames(experimental_design) = c(&#39;Group 1&#39;, &#39;Group 2&#39;, &#39;Group 3&#39;, &#39;Group 4&#39;) library(kableExtra) library(reticulate) kable(experimental_design[1:4,1:2] #, &quot;latex&quot; , caption = &#39;Treatment groups of the mice that were either controls or treated with ionizing radiation to determine reaction of p53.&#39; #,booktabs = T ) %&gt;% kable_styling(latex_options = c(&quot;striped&quot;, &quot;repeat_header&quot;)) (#tab:experimental_design)Treatment groups of the mice that were either controls or treated with ionizing radiation to determine reaction of p53. Genotype Treatment Group 1 p53 Mock Group 2 C57/Bl6 Mock Group 3 p53 IR Group 4 C57/Bl6 IR This document will contain 2 different pipelines: The first one is going to be using the genome to map the reads too, and the secod is going to be de novo. Genome De novo ##Genome Mapping {#genome-mapping} The pipeline used in this analysis used conda on South Dakota State University’s High Performance Computing cluster to run the programs FastQC, Trimmomatic, and Tophat. This is different than previous RNA-Seq analyses where I used my workstation pc with Ubuntu 18.04 to run FastQC, Trimmomatic, HiSat2, HTSeq, and DESeq2 locally. Also, the previous RNA-Seq alayses were of Soybean with treatment combinations of mycorrhizae and rhizobia inoculation. 5.2 Cleaning Data 5.2.1 Programs used? FastQC Trimmomatic 0.39 Bowtie 2.2.5.0 Tophat 2.1.1 STAR Cufflinks Kraken2 MultiQC featureCount 5.2.1.1 Picking the right node To find a node that we can use on our own, we need to see which nodes are already allocated to jobs and which ones are idle. To do this, we can run sinfo. We want to pick one of the nodes that are marked ‘idle’ so we get the whole thing and we aren’t interrupting someone elses job. For the sake of this exercise, lets work on big-mem. Once a node that is idle has been found, you can ssh into it by typing ssh -X big-mem00# where # is the node number. ssh -X big-mem005 Once on the node, the modules will have to be pulled from the shared folder again, otherwise we will be left with very basic ones. NOTE: if running programs that are in a personal folder such as miniconda (these examples), it is not necessary to add the other modules. module use /cm/shared/modulefiles_local/ After loading the modules you can use it just as you would any other command line. 5.2.1.2 Creating slurm scripts When running on a cluster, it can sometime be difficult to find open nodes with the resources needed to run the jobs that we have. Making a slurm script is really easy. Fist we make a new file with the touch command. touch commands.slurm Now in our directory we have the file commands.slurm which we can edit to hold our code in. We can edit it with the vi command. vi commands.slurm We have a few things that we need to put in the file header so slurm knows what to do with our commands. #!/bin/bash #SBATCH --job-name=example #SBATCH --nodes=1 #SBATCH --ntasks-per-node=10 #SBATCH --output=job-%j-%N.log #SBATCH --partition=bigmem #SBATCH --time=10:00:00 When we break this down, we see –job-name which is what we will see when we look at whats running later, –nodes is the number of nodes we have, –ntasks-per-node is the number of cores that we are requesting to have allocated, –output is the output log file of the job (here it names the output file with the job number and the node that we used), –partition here is requesting a big-mem node but compute can also be used, and finally –time is how long we are requesting the allocation for. If the time runs out before the job is done I believe that it just kills the job even if not finished so we need to think a little about how much time to set. If the time is set too low, the job is killed and if the time is set too long, we may face issues with getting the node allocated to us. To submit a job we can use sbatch commands.slurm and then we have the job ID. To check the status of our submission we use sbatch and then it shows all of the submitted jobs and how long they have been running along with the name that we set in the script. 5.2.2 Acquiring sequences To download the sequences from the sequence read archive (SRA), the SRA Toolkit was used. The downloading of the files took a very long time, so this was left to run over night. The –gzip was used to keep the files a relatively small, although this can be left out to download uncompressed files, and –split-files was used to split the forward read from the reverse read for paired end read trimming through Trimmomatic. ~/tools/sratoolkit.2.9.6-1-centos_linux64/bin/fastq-dump --gzip --split-files SRR2121770 This is an example of the single file, but the above code needed to be ran for all of the following SRA numbers: SRR2121770 SRR2121771 SRR2121774 SRR2121775 SRR2121778 SRR2121779 SRR2121780 SRR2121781 SRR2121786 SRR2121787 SRR2121788 SRR2121789 The results from downloading with –split-files gives 2 files per SRR, as mentioned before, one forward and one reverse. The suffix of the split files is one with _1.fastq.gz and another with _2.fastq.gz. 5.2.3 FastQC FastQC can be run on all of the read files by using the wild card (*) as in *.fastq.gz. This prevents the need to hard code each individual read file into a FastQC command, which saves a lot of time since there are 24 read files in total for these 12 samples. ~/miniconda2/bin/fastqc *.fastq.gz The output from running FastQC is a zipped folder and an HTML file for each of the .gz files in the folder. The HTML document looks something like this: ![FastQC of Raw SRR2121770_1.fastq.gz Read](./TopFastQCRaw.PNG) This is just the top of the file, and every category under the Summary heading has a graph that shows how the read quality looks for that particular metric. These reports can give insight into whether the reads are of decent quality or if the quality is poor. The raw reads we have here all passed for adapter content and sequence length distribution and everything failed per base sequence content. SRR2121770, SRR2121771, SRR2121774, SRR2121775, SRR2121788, SRR2121781-2, and SRR2121789-1 were fairly decent quality reads. SRR2121778, SRR2121779, SRR2121780, SRR2121786, SRR2121787, SRR2121781-1, and SRR2121789-2 were of fairly lower quality (failing 3 or more in both reads. All of them failed both per base sequence quality and per tile sequence quality. 5.2.3.1 MultiQC First lets install multiqc with conda. The command for this is conda install -c bioconda multiqc. When that is finished, we can run MultiQC in the folder with the QC files (they should be moved into a folder alone so things don’t get cluttered later on in the analyses). ~/miniconda/bin/multiqc . When MultiQC is finished running, there will be a new folder called multiqc_data where the summaries are stored. Now lets go back up a level where our raw data folder and fastqc folder is and make a new folder for all of our MultiQC data. We will copy the FastQC output from MultiQC to this new folder. mkdir MultiQC_All cp RawQC/multiqc_data/multiqc_fastqc.txt MultiQC_All/ 5.2.4 Trimming with Trimmomatic Conda was used again to run Trimmomatic. This isn’t as easy as using the wildcard like with FastQC because each output has to be personalized for the read files that are input into Trimmomatic. Also, we have to make sure that the adapter sequences are in the same folder that we are running so we can refer to them easily when calling the Trimmomatic program. In this case, we are using the TruSeq3-PE-2.fa adapter sequences For example: ~/miniconda2/bin/trimmomatic PE SRR2121770_1.fastq.gz SRR2121770_2.fastq.gz 770_fp.fq.gz 770_fu.fq.gz 770_rp.fq.gz 770_ru.fq.gz ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10:2keepBothReads LEADING:3 TRALING:3 MINLEN:36 &amp; This would be repeated for each of the pairs (12 in total). We are trimming paired-end reads with the TruSeq3-PE-2 adapters. We are chopping off the first and last 3 bases and if we end up with a sequence less than 36 bases, we get rid of it. We want to make sure that there are enough bases in a read to work with. These parameters can be tweaked for possibly better end results with less being discarded. When Trimmomatic is finished running, it will out put the total number of reads, the total number from both the forward and reverse reads that are kept, the number of only forward reads kept, the number of only reverse reads kept, and the number of discarded reads. The highest number of reads dropped was from trimming SRR2121786, where 20.55% dropped. Most reads were between 5% and 10% dropped. SRR2121786, SRR2121787, and SRR2121779 had sequence drops greater than 15%. The trimmed reads can be analyzed again with FastQC to see how well the trimming worked to make the file better quality. After running FastQC on the trimmed files we see that the quality of those that were really bad quality were improved. There were a few different metrics throughout all of the files that bounced from a warning before the failing, or from passing before to a warning, and so forth, overall creating better quality read files. 5.2.5 Alignment 5.2.5.1 Using Tophat Tophat can be installed using the same conda install (conda install -c bioconda tophat). When this is finished installing, then we will need to get the mouse genome from the Johns Hopkins Univeristy Center for computational BIology. The version of the mouse genome that I am using here is the NCBI build37.2. Instead of downloading this from the website and having to move it to the cluser, I will just download it using wget into the folder that has the raw reads, trimmed reads, and the FastQC files. wget ftp://igenome:G3nom3s4u@ussd-ftp.illumina.com/Mus_musculus/NCBI/build37.2/Mus_musculus_NCBI_build37.2.tar.gz This will take a long time to download because the file is a little less than 16GB zipped. We notice here that we have a zipped tar file. To make this file easier to use, lets unzip it. tar zxvf Mus_muculus_NCBI_build37.2.tar.gz Since Tophat is requiring *.bt21 files (large index) and the files downloaded for the genome above are only small index files, we have to create a large index using bowtie2-build. For this, lets navigate to the WholeGenomeFasta folder within the extracted folder and then run bowtie2-build. ~/miniconda2/bin/bowtie2-build --large-index genome.fa genome This process took about 26 minutes to run. Now lets copy the index files to a folder close to our reads so we can access them easier, rather than having to refer to the longer path where we build them. After they are copied to a new folder closer to our working directory, I went ahead and unzipped the trimmed read files to try and make the Tophat faster but it turned out not to work. The multicore call with -p didn’t use more cores than 1 until bowtie2-align-s, then 20 cores were used. ~/miniconda2/bin/tophat --no-converage-search -p 20 -G Mus_musculus/NCBI/build37.2/Annotation/Archives/archive-2015-07-17-14-32-40/Genes/genes.gtf -0 770_thout ./Index/genome 770_fp.fq.gz 770_rp.fq.gz 770_fu.fq 770_ru.fq This run took almost 3 hours to complete.. Running with 80 cores rather than 20 cores took just 4 minutes less, so the whole process must be limited by a single core and the core clock speed. The process does use close to 8,000% at its peak so there is a benefit to multicore, just isn’t very scalable. 5.2.5.2 Using STAR STAR can be installed the same way as the previous programs with conda install (conda install -c bioconda star). In order to run STAR, we need to creaate indices just like with tophat, but STAR has this built in. I’m going to be using the same genome and GTF file as previously downloaded, but Dr. Ge uses a different zipped genome from the gencode database. ~/miniconda2/bin/STAR \\ --runThreadN 80 \\ --runMode genomeGenerate \\ --genomeDir starIndex \\ --genomeFastaFiles Index/genome.fa \\ #same when we made the bowtie indices --sjdbGTFfile Mus_musculus/NCBI/build37.2/Annotation/Archives/archive-2015-07-17-14-32-40/Genes/genes.gtf With the index files made, we can start aligning with STAR. It’s important here than we only pick the paired end reads and not use all of the reads. Tophat is able to use all 4 reads but STAR doesn’t allow that, so we need to make sure that we feed in the large files from trimming. ~/miniconda2/bin/STAR --runThreadN 80 --genomeDir starIndex --readFilesIn 770_fp.fq 770_rp.fq --outFilterIntronMotifs RemoveNoncanonical --outFileNamePrefix 2121770 --outSAMtype BAM SortedByCoordinate 5.2.6 Assembling transcripts with Cufflinks Once STAR is done running, we can assemble the transcripts with Cufflinks. This can also be installed with conda install (conda install -c bioconda cufflinks). ~/miniconda2/bin/cufflinks -p 20 -o SRR2121771_clout --library-type fr-firststrand 2121770Aligned.sortedByCoord.out.bam 5.2.7 Checking for Contamination 5.2.7.1 PhiX contamination Now we will look at what kind of contamination we are looking at. When samples are sequenced with Illumina, a PhiX control is run along side them. This control is for cluster generation, sequencing, alignment, and calibration for cross-talk matrix generation. We will use Bowtie to create a file to determine the PhiX contamination level. ~/miniconda2/bin/bowtie2 -p 20 -x PhiX/Illumina/RTA/Sequence/Bowtie2Index/genome \\ -1 TrimmedReads/770_fp.fq -2 TrimmedReads/770_rp.fq -S phix.sam &amp;&gt; PhiXout/SRR2121770_phix.out When the job is done running, the output file will show how much PhiX contamination we have. For example, lookin at the SRR2121770_phix.out created above, we see that 0.11% of the reads aligned with PhiX. The lower this value the better. 5.2.7.2 rRNA Sequences To retreive the rRNA sequences for mouse, we need to search the taxonomy database on NCBI for Mus musculus. Click on Mus musculus on the next page, and then the top Mus musculus at the head of the list. Now, select the top subtree link in the Nucleotide database. Select rRNA sequences on the left side of the page and download full list just downloading with Send &gt; Complete Record &gt; File &gt; FASTA &gt; Create File. Drag the file using WinSCP to the raw folder on the cluster and rename it to rRNA.fa. We are going to need to install bwa with conda in order to get the alignments to work. This can be done with conda install -c bioconda bwa. Following this, we will need to make indixes for the rRNA that we downloaded. To make this more clean, lets make a directory for the rRNA sequences that we downloaded and the indices that we make. mkdir rRNA Then we move the rRNA.fa to the new rRNA folder with WinSCP and then we can run the bwa. time ~/miniconda2/bin/bwa mem -t 20 rRNA/rRNA.fa TrimmedReads/770_fp.fq TrimmedReads/770_rp.fq &gt; rnaAlign/770_rna.sam When we are done creating the new *.sam files for all of the forward/reverse read combinations, we can use samtools to convert the *.sam file to *.bam files which are essentially the same file just that sam is easier for us to look at while bam is binary. Samtools can be installed with conda install -c bioconda samtools. ~/miniconda2/bin/samtools view -@ 10 -bS -o rnaAlign/770_rna.bam rnaAlign/770.sam Now in the rnaAlign folder we have our sam and bam file for each of the libraries. Lets create an output file with flagstat. ~/miniconda2/bin/samtools flagstat -@ 10 rnaAlign/770_rna.out Wihtin this file we will be able to see the summary of our alignments to the rRNA file that we downloaded from NCBI. #From 770_rna.out 205559289 + 0 in total (QC-passed reads + QC-failed reads) 0 + 0 secondary 85 + 0 supplementary 0 + 0 duplicates 4265179 + 0 mapped (2.07% : N/A) 205559204 + 0 paired in sequencing 102779602 + 0 read1 102779602 + 0 read2 4151684 + 0 properly paired (2.02% : N/A) 4187608 + 0 with itself and mate mapped 77486 + 0 singletons (0.04% : N/A) 4222 + 0 with mate mapped to a different chr 1026 + 0 with mate mapped to a different chr (mapQ&gt;=5) 5.2.7.3 Bacterial contamination In order to find out the contamination, we need to install Kraken2 with conda install -c bioconda kraken2 and download a pre-built database containing bacteria, archaea, and viral sequences. The database we are going to download only contains about 5% of k-mers from the original database (but directions are sort of lacking to build an entirely new database). More information can be found at https://ccb.jhu.edu/software/kraken/ for the pre-built databases. Using the code in the next chunk will download the 8GB database and then extract the files so we can use them with the Kraken2 program. Lets do this in the main project folder. wget ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/minikraken2_v2_8GB_201904_UPDATE.tgz tar xzf minikraken2_v2_8GB_201904_UPDATE.tgz Now lets make a directory for the output. mkdir krakenOut We can call Kraken with the extracted database folder and point it to the location of out paired end reads from trimming and to the output folder that we just created for the outputs. ~/miniconda2/bin/kraken2 --db minikraken2_v2_8GB_201904_UPDATE/ --output krakenOut/770.out --threads 10 --paired TrimmedReads/770_fp.fq TrimmedReads/770_rp.fq When Kraken is done running, it will print out the number (and percentage) of reads that were classified. In this case, we have used 102779602 sequences, of which 19142843 sequences were classified (18.63%) and 83636759 sequences were unclassified (81.37%). My interpretation of this is that 18.63% of the reads are possibly from microbial cell contamination. 5.2.8 Counting Transcripts Since we have the bam files from the alignments of the different samples, we can count the features for each and get the transcipt counts using featureCounts form conda install -c bioconda/label/cf201901 subread. The genome and annotations that we previously downloaded were from genome mm9 so we have to specify to featureCounts what we want to actually count. FeatureCounts defaults to using gene_id which our output bam files don’t have described correctly for featureCounts to read them. This is a single line of code because we can use a wildcard to run through all of the bam files. #Move to the Star Alignment output folder for a working directory cd StarOut ~/miniconda2/bin/featureCounts -a /gpfs/scratch/alex.soupir/Mus/raw/Mus_musculus/NCBI/build37.2/Annotation/Archives/archive-2015-07-17-14-32-40/Genes/genes.gtf -g &#39;transcript_id&#39; -o readCounds.txt *bam With the files that we are working with, this will take between 3.5 minutes to 5 minutes per bam file. The output will be a file that can be imported into excel and saved as csv which we then can work with in R. 5.2.8.1 Final QC of cleaning the data Lets look at the data that we have collected from all of the MultiQC runs that we had with initial FastQC, Trimmomatic, STAR alignment, PhiX contamination, rRNA contamination, and the final feature counts. qc = read.csv(&#39;Whole Data QC.csv&#39;, header=TRUE, na.strings=&quot;&quot;) kable(qc) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;800px&quot;) 5.3 Differentially Expressed Sequence Identification Programs Used R RStudio Packages readr (install.packages(‘readr’)) limma (BiocManager::install(‘limma’)) DESeq2 (BiocManager::install(‘DESeq2’)) dplyr (install.packages(“dplyr”)) ggplot2 (install.packages(“ggplot2”)) gplots (install.packages(“gplots”)) Annotations (BiocManager::install(‘AnnotationDbi’)) org.Hs.eg.db (BiocManager::install(‘org.Hs.eg.db’)) This is for Human org.Mm.eg.db (BiocManager::install(‘org.Mm.eg.db’)) This is for Mouse ggrepel (install.packages(“ggrepel”)) ReportingTools (BiocManager::install(‘ReportingTools’)) GO.db (BiocManager::install(‘GO.db’)) GOstats (BiocManager::install(‘GOstats’)) pathview (BiocManager::install(‘pathview’)) gage (BiocManager::install(‘gage’)) gageData (BiocManager::install(‘gageData’)) select (BiocManager::install(‘Select’)) With these, you most certainly will have to step through each and install extra things when you start calling the packages. Take it step by step to ensure that each dependency is installed. 5.3.1 Analyzing Reads Counts When the count file is completed, we can import it into R and start working with it to determine differentially expressed genes. First we will import it into R library(limma) library(DESeq2) library(dplyr) library(readr) countData = read_csv(&quot;readCounts.csv&quot;, skip = 1) This gives us our dataframe from out featureCounts program, but if we look at the data we see that featureCounts added some extra information that characterizes each gene_id. kable(head(countData)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;320px&quot;) We also need to set out row names to the gene_id. We will do some data frame manipulation and then look at the data again. countData = as.data.frame(countData) rownames(countData) = countData$Geneid countData = countData[,-c(1:6)] kable(head(countData)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) 5.3.1.1 Quick Data Exploration dim(countData) kable(summary(countData)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) Let’s also go ahead and change the names to describe out data a little better. columns = c(&#39;Trp53m_mock_1&#39;, &#39;Trp53m_mock_2&#39;, &#39;Trp53m_4h7Gy_1&#39;, &#39;Trp53m_4h7Gy_2&#39;, &#39;Trp53p_mock_1&#39;, &#39;Trp53p_mock_2&#39;, &#39;Trp53p_mock_3&#39;, &#39;Trp53p_mock_4&#39;, &#39;Trp53p_4h7Gy_1&#39;, &#39;Trp53p_4h7Gy_2&#39;, &#39;Trp53p_4h7Gy_3&#39;, &#39;Trp53p_4h7Gy_4&#39;) colnames(countData) = columns Here we have to make sure that we convert the +/+ and -/- to characters. These characters par(mar=c(8,4,4,1)+0.1) barplot( colSums(countData)/1e6, col=&quot;green&quot;,las=3,main=&quot;Total read counts (millions)&quot;, ylab=&quot;Total read counts in millions&quot;) hist(countData[,1], br=200, xlab=&quot;Number of Reads Counts per Feature&quot;, main=&quot;Histogram of Read Counts for Trp53-/- Mock&quot;) We can see that our count data is highly skewed to the right. This is a great case for using log transformation! logCountData = log2(1+countData) par(mfrow = c(1, 2), mar=c(8,4,4,1)) # two columns hist(logCountData[,1], main=&quot;Histogram of Log Read Counts&quot;, xlab=&quot;Log transformed counts&quot;) boxplot(logCountData,las=3, main=&quot;Boxplot of Log Read Counts&quot;) x &lt;- logCountData myColors = rainbow(dim(x)[2]) plot(density(x[,1]),col = myColors[1], lwd=2, xlab=&quot;Expresson values&quot;, ylab=&quot;Density&quot;, main= &quot;Distribution of transformed data&quot;, ylim=c(0, max(density(x[,1])$y)+.02 ) ) for( i in 2:dim(x)[2] ) lines(density(x[,i]),col=myColors[i], lwd=2) legend(&quot;topright&quot;, cex=1.1,colnames(x), lty=rep(1,dim(x)[2]), col=myColors ) plot(logCountData[,1],logCountData[,2], xlab=&quot;Trp53-/- mock replication 1&quot;, ylab=&quot;Trp53-/- mock replication 2&quot;) 5.3.1.2 Filtering, Normalization, and Trasformation using DESeq2 We have to make the experiment design into a small dataframe so we can tell DESeq how we want to analyze the data. Here will will make a small table that has the rep names that we changed the column names to previously, and then a column for which columns are Trp53+/+ or Trp53-/-, and which columns were control mice and which columns were treated with ionizing radiation. detectGroups &lt;- function (x){ # x are col names tem &lt;- gsub(&quot;[0-9]*$&quot;,&quot;&quot;,x) # Remove all numbers from end #tem = gsub(&quot;_Rep|_rep|_REP&quot;,&quot;&quot;,tem) tem &lt;- gsub(&quot;_$&quot;,&quot;&quot;,tem); # remove &quot;_&quot; from end tem &lt;- gsub(&quot;_Rep$&quot;,&quot;&quot;,tem); # remove &quot;_Rep&quot; from end tem &lt;- gsub(&quot;_rep$&quot;,&quot;&quot;,tem); # remove &quot;_rep&quot; from end tem &lt;- gsub(&quot;_REP$&quot;,&quot;&quot;,tem) # remove &quot;_REP&quot; from end return( tem ) } groups = as.character ( detectGroups( colnames( countData ) ) ) groups p53 = c(&quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;) treatment = c(&quot;control&quot;, &quot;control&quot;, &quot;IR&quot;, &quot;IR&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;IR&quot;, &quot;IR&quot;, &quot;IR&quot;, &quot;IR&quot;) colData = cbind(colnames(countData), p53 ) colData colData = as.data.frame(cbind(colnames(countData), p53, treatment)) colData str(colData) Creating a DESeq Dataset dds = DESeqDataSetFromMatrix(countData=countData, colData=colData, design= ~ p53+treatment+p53*treatment) # note that the study design is changed. dds = DESeq(dds) # main function nrow(dds) Filtering: we will only keep rows that have a sum count between all samples greater than 5. This will remove most of the genes that mostly have “0” counts. dds &lt;- dds[ rowSums(counts(dds)) &gt; 5, ] nrow(dds) Regularized log transformation - used for clustering rld &lt;- rlog(dds, blind = FALSE) kable(head(assay(rld), 6)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) Variance Stabilizing Transformation 5.4 Interactions cause a difference between the lfc betwen pooled data, e.g. p53+/+ (control and IR) and p53-/- (control and IR) vsd &lt;- vst(dds, blind = FALSE) kable(head(assay(vsd), 6)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) For the log2 approach, we need to first estimate size factors to account for sequencing depth, and then specify normalized=TRUE. Sequencing depth correction is done automatically for the rlog and the vst. Size Factor dds &lt;- estimateSizeFactors(dds) kable(sizeFactors(dds)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;300px&quot;, height = &quot;520px&quot;) We will first look at the log transformed data slog &lt;- log2(counts(dds, normalized=TRUE)+1) kable(head(slog)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) par(mfrow = c(1, 3)) # 3 columns plot(slog[,1],slog[,2]) plot(assay(rld)[,1],assay(rld)[,2]) plot(assay(vsd)[,1],assay(vsd)[,2]) As the log transformation constant increases, the information of the data is lost. par(mfrow = c(1, 3)) # 3 columns slog &lt;- log2(counts(dds, normalized=TRUE)+1) plot(slog[,1],slog[,2]) slog &lt;- log2(counts(dds, normalized=TRUE)+4) plot(slog[,1],slog[,2], xlim=c(0,20)) slog &lt;- log2(counts(dds, normalized=TRUE)+20) plot(slog[,1],slog[,2], xlim=c(0,20)) library(&quot;dplyr&quot;) library(&quot;ggplot2&quot;) df &lt;- bind_rows( as_data_frame(slog[,1:2]) %&gt;% mutate(transformation = &quot;log2(x + 1)&quot;), as_data_frame(assay(rld)[, 1:2]) %&gt;% mutate(transformation = &quot;rlog&quot;), as_data_frame(assay(vsd)[, 1:2]) %&gt;% mutate(transformation = &quot;vst&quot;)) colnames(df)[1:2] &lt;- c(&quot;x&quot;, &quot;y&quot;) ggplot(df, aes(x = x, y = y)) + geom_hex(bins = 80) + coord_fixed() + facet_grid( . ~ transformation) 5.4.1 Exploratory Data Analysis PCA plot plotPCA(rld, intgroup = c(&quot;p53&quot;, &quot;treatment&quot;)) + theme(aspect.ratio=1) A prettier PCA plot created with GGPlot2 pca.object &lt;- prcomp(t(assay(rld))) # PCA pcaData = as.data.frame(pca.object$x[,1:2]); pcaData = cbind(pcaData,detectGroups(colnames(assay(rld)) )) colnames(pcaData) = c(&quot;PC1&quot;, &quot;PC2&quot;, &quot;Type&quot;) percentVar=round(100*summary(pca.object)$importance[2,1:2],0) #plot p=ggplot(pcaData, aes(PC1, PC2, color=Type, shape = Type)) + geom_point(size=5) p=p+xlab(paste0(&quot;PC1: &quot;,percentVar[1],&quot;% variance&quot;)) p=p+ylab(paste0(&quot;PC2: &quot;,percentVar[2],&quot;% variance&quot;)) p=p+ggtitle(&quot;Principal component analysis (PCA)&quot;)+coord_fixed(ratio=1.0)+ theme(plot.title = element_text(size = 16,hjust = 0.5)) + theme(aspect.ratio=1) + theme(axis.text.x = element_text( size = 16), axis.text.y = element_text( size = 16), axis.title.x = element_text( size = 16), axis.title.y = element_text( size = 16) ) + theme(legend.text=element_text(size=16)) print(p) Multidimensional Scaling Plot dist2 &lt;- function(x, ...) # distance function = 1-PCC (Pearson&#39;s correlation coefficient) as.dist(1-cor(t(x), method=&quot;pearson&quot;)) fit = cmdscale( dist2(t(assay(rld))) , eig=T, k=2) mdsData &lt;- as.data.frame(fit$points[,1:2]); mdsData &lt;- cbind(mdsData,detectGroups(colnames(assay(rld))) ) colnames(mdsData) = c(&quot;x1&quot;, &quot;x2&quot;, &quot;Type&quot;) p&lt;-ggplot(mdsData, aes(x1, x2, color=Type, shape = Type)) + geom_point(size=5) p=p+xlab(&quot;Dimension 1&quot;) p=p+ylab(&quot;Dimension 2&quot;) p=p+ggtitle(&quot;Multidimensional scaling (MDS)&quot;)+ coord_fixed(ratio=1.)+ theme(plot.title = element_text(hjust = 0.5)) + theme(aspect.ratio=1) + theme(axis.text.x = element_text( size = 16), axis.text.y = element_text( size = 16), axis.title.x = element_text( size = 16), axis.title.y = element_text( size = 16) ) + theme(legend.text=element_text(size=16)) print(p) Creating a heatmap library(gplots) hclust2 &lt;- function(x, method=&quot;average&quot;, ...) # average linkage in hierarchical clustering hclust(x, method=method, ...) n=100 # number of top genes by standard deviation x = assay(rld) if(n&gt;dim(x)[1]) n = dim(x)[1] # max as data x = x[order(apply(x,1,sd),decreasing=TRUE),] # sort genes by standard deviation x = x[1:n,] # only keep the n genes # this will cutoff very large values, which could skew the color x=as.matrix(x[1:n,])-apply(x[1:n,],1,mean) cutoff = median(unlist(x)) + 4*sd (unlist(x)) x[x&gt;cutoff] &lt;- cutoff cutoff = median(unlist(x)) - 4*sd (unlist(x)) x[x&lt; cutoff] &lt;- cutoff groups = detectGroups(colnames(x) ) groups.colors = rainbow(length(unique(groups) ) ) lmat = rbind(c(5,4),c(0,1),c(3,2)) lwid = c(1.5,4) lhei = c(1,.2,4) heatmap.2(x, distfun = dist2,hclustfun=hclust2, col=greenred(75), density.info=&quot;none&quot;, trace=&quot;none&quot;, scale=&quot;none&quot;, keysize=.5 ,key=T, symkey=F ,ColSideColors=groups.colors[ as.factor(groups)] ,margins=c(8,12) ,cexRow=1 ,srtCol=45 ,cexCol=1. # size of font for sample names ,lmat = lmat, lwid = lwid, lhei = lhei ) 5.4.2 Identification of Differentially Expressed Genes dds &lt;- DESeq(dds) res &lt;- results(dds) kable(head(res)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) DESeq2 uses the Benjamini-Hochberg (BH) adjustment (Benjamini and Hochberg 1995) as implemented in the base R p.adjust function res &lt;- results(dds, alpha = 0.5, lfcThreshold=0.01) summary(res) Now lets sort genes by fold change res &lt;- res[order(abs( res$log2FoldChange), decreasing=TRUE),] kable(head(res)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) MA Plot Show the significant genes. The lower the average read counts for all samples and the higher the variation between the samples, the less significant those genes are. DESeq2::plotMA(res, ylim = c(-5, 5)) Volcano plot library(dplyr) res1 = as.data.frame(res) # add a new column using the mutate function in dplyr res1 = mutate(res1, sig=ifelse(res1$padj&lt;0.05, &quot;FDR&lt;0.05&quot;, &quot;Not Sig&quot;)) res1[which(abs(res1$log2FoldChange)&lt;0.5),&#39;sig&#39;] &lt;- &quot;Not Sig&quot; p = ggplot(res1, aes(log2FoldChange, -log10(padj))) + geom_point(aes(col=sig)) + scale_color_manual(values=c(&quot;red&quot;, &quot;black&quot;)) p 5.4.3 Gene Annotations Plot counts of top gene topGene &lt;- rownames(res)[1] plotCounts(dds, gene = topGene, intgroup=c(&quot;p53&quot;, &quot;treatment&quot;)) Here we see an interesting point or our normalized counts under the Trp53p_4h7Gy group that seems to be extremely high, while the other 3 replicates are around 0.5. I cannot get this portion to work, however. I get an error stating that None of the keys entered are valid keys for ‘SYMBOL.’ Let’s look at the keys that we have to work with for our res file from DESeq2 head(row.names(res)) Now we need to find the same key in the Mm database. library(AnnotationDbi) library(org.Mm.eg.db) columns(org.Mm.eg.db) #key = gsub(&quot;\\\\..*&quot;,&quot;&quot;, row.names(res)) res$symbol &lt;- gsub(&quot;\\\\..*&quot;,&quot;&quot;, row.names(res)) #res$symbol &lt;- gsub(&quot; &quot;,&quot;&quot;,row.names(res)) message(&quot;Ensembl IDs&quot;) key.en = keys(org.Mm.eg.db, keytype=&quot;ENSEMBL&quot;) head(key.en) cat(&quot;\\n\\n&quot;) message(&quot;SYMBOL names&quot;) key.sy = keys(org.Mm.eg.db, keytype=&quot;SYMBOL&quot;) head(key.sy) These are ENSEMBL symbols, so we need to designate that when looking for the genes that we have. res$ensembl &lt;- gsub(&quot;\\\\..*&quot;,&quot;&quot;, row.names(res)) res$entrez &lt;- mapIds(org.Mm.eg.db, keys= res$ensembl, column=&quot;ENTREZID&quot;, keytype=&quot;ENSEMBL&quot;, #Out ID is ENSMBL multiVals=&quot;first&quot;) res$symbol &lt;- mapIds(org.Mm.eg.db, keys= res$ensembl, column=&quot;SYMBOL&quot;, keytype=&quot;ENSEMBL&quot;, #Out ID is ENSMBL multiVals=&quot;first&quot;) write.csv(res, file = &quot;results.csv&quot;) kable(head(res)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) Let’s make a file with just the genes with an adjusted p-value &lt; 0.5 resSig = as.data.frame(subset(res,padj&lt;0.5) ) resSig = resSig[order(resSig$log2FoldChange,decreasing=TRUE),] head(resSig) write.csv(resSig,&quot;SigGenes.csv&quot;) Here is a volcano plot that shows the symbol that we created at each point. library(dplyr) res1 = as.data.frame(res) # add a new column using the mutate function in dplyr res1 = mutate(res1, sig=ifelse(res1$padj&lt;0.5, &quot;FDR&lt;0.05&quot;, &quot;Not Sig&quot;)) res1[which(abs(res1$log2FoldChange)&lt;1),&#39;sig&#39;] &lt;- &quot;Not Sig&quot; p = ggplot(res1, aes(log2FoldChange, -log10(pvalue))) + geom_point(aes(col=sig)) + scale_color_manual(values=c(&quot;red&quot;, &quot;black&quot;)) p+geom_text(data=filter(res1, padj&lt;1e-50), aes(label=symbol)) library(dplyr) # Install ggrepel package if needed # install.packages(&quot;devtools&quot;) # devtools::install_github(&quot;slowkow/ggrepel&quot;) library(ggrepel) # &quot;repels&quot; overlapping text p+geom_text_repel(data=filter(res1, abs(log2FoldChange)&gt;10 | padj &lt; 1e-50 ), aes(label=symbol)) library(&quot;ReportingTools&quot;) htmlRep &lt;- HTMLReport(shortName=&quot;report&quot;, title=&quot;My report&quot;, reportDirectory=&quot;./report&quot;) publish(resSig, htmlRep) url &lt;- finish(htmlRep) #browseURL(url) 5.5 7. GO Enrichment analysis using GOstats Here we will do a GO Enrichment analysis for genes that have a decreased fold-change of 5 or more library(GO.db) library(GOstats) selectedGenes = unique(resSig[resSig$log2FoldChange&gt;5,&#39;entrez&#39;]) # upregulated genes universeGenes = unique( mapIds(org.Mm.eg.db, keys= res$ensembl, column=&quot;ENTREZID&quot;, keytype=&quot;ENSEMBL&quot;, #Out ID is ENSMBL multiVals=&quot;first&quot;) ) hgCutoff &lt;- 0.001 params &lt;- new(&quot;GOHyperGParams&quot;, geneIds=selectedGenes, universeGeneIds=universeGenes, annotation=&quot;org.Mm.eg.db&quot;, ontology=&quot;BP&quot;, pvalueCutoff=hgCutoff, conditional=FALSE, testDirection=&quot;over&quot;) hgOver &lt;- hyperGTest(params) summary(hgOver)[1:10,] summary(hgOver)[1:10,c(&quot;GOBPID&quot;,&quot;Pvalue&quot;,&quot;Term&quot;)] params1 &lt;- params ontology(params1) &lt;- &quot;CC&quot; hgOver &lt;- hyperGTest(params1) summary(hgOver)[1:10,c(&quot;GOCCID&quot;,&quot;Pvalue&quot;,&quot;Term&quot;)] params1 &lt;- params ontology(params1) &lt;- &quot;MF&quot; hgOver &lt;- hyperGTest(params1) summary(hgOver)[1:10,c(&quot;GOMFID&quot;,&quot;Pvalue&quot;,&quot;Term&quot;)] 5.5.1 GO Enrichment analysis of downregulated genes Next we will have a look at the genes that are upregulated by a fold-change of 5 or greater. selectedGenes = unique(resSig[resSig$log2FoldChange&lt;5,&#39;entrez&#39;]) # upregulated genes params &lt;- new(&quot;GOHyperGParams&quot;, geneIds=selectedGenes, universeGeneIds=universeGenes, annotation=&quot;org.Mm.eg.db&quot;, ontology=&quot;BP&quot;, pvalueCutoff=hgCutoff, conditional=FALSE, testDirection=&quot;over&quot;) hgOver &lt;- hyperGTest(params) summary(hgOver)[1:10,c(&quot;GOBPID&quot;,&quot;Pvalue&quot;,&quot;Term&quot;)] 5.6 8. Pathway analysis using expression data # bioconductor packages # source(&quot;https://bioconductor.org/biocLite.R&quot;); # biocLite(c(&quot;pathview&quot;,&quot;gage&quot;,&quot;gageData&quot;)) library(pathview) library(gage) 5.6.1 Prepare data foldchanges = res$log2FoldChange names(foldchanges) = res$entrez head(foldchanges) library(gageData) data(go.sets.mm) data(go.subs.mm) gobpsets = go.sets.mm[go.subs.mm$BP] gobpres = gage(foldchanges, gsets=gobpsets, same.dir=TRUE) #lapply(gobpres, head) message(&quot;Greater&quot;) kable(head(gobpres$greater)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) message(&quot;Less&quot;) kable(head(gobpres$less)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) message(&quot;Stats&quot;) kable(head(gobpres$stats)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) 5.6.2 KEGG pathways library(gageData) data(kegg.sets.mm) data(sigmet.idx.mm) kegg.sets.mm = kegg.sets.mm[sigmet.idx.mm] #head(kegg.sets.mm, 3) message(&quot;Greater&quot;) kable(head(kegg.sets.mm$greater)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) message(&quot;Less&quot;) kable(head(kegg.sets.mm$less)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) message(&quot;Stats&quot;) kable(head(kegg.sets.mm$stats)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) # Get the results keggres = gage(foldchanges, gsets=kegg.sets.mm, same.dir=TRUE) # Look at both up (greater), down (less), and statatistics. #lapply(keggres, head, n=10) message(&quot;Greater&quot;) kable(head(keggres$greater)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) message(&quot;Less&quot;) kable(head(keggres$less)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) message(&quot;Stats&quot;) kable(head(keggres$stats)) %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;1000px&quot;, height = &quot;300px&quot;) # Get the pathways keggrespathways = data.frame(id=rownames(keggres$less), keggres$less) %&gt;% tbl_df() %&gt;% filter(row_number()&lt;=5) %&gt;% .$id %&gt;% as.character() keggrespathways # Get the IDs. keggresids = substr(keggrespathways, start=1, stop=8) keggresids # Define plotting function for applying later plot_pathway = function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=&quot;mmu&quot;, new.signature=FALSE) # plot multiple pathways (plots saved to disk and returns a throwaway list object) tmp = sapply(keggresids, function(pid) pathview(gene.data=foldchanges, pathway.id=pid, species=&quot;mmu&quot;)) 5.6.3 Pathway and regulation of genes for Oxidative phosphorylation. ![Oxidative Phosphorylation](./Mouse_RNA_Seq_p53_genotoxic_files/figure-html/mmu00190.pathview.png) 5.6.4 Pathway and regulation of genes for Glycosylphosphatidylinositol(GPI)-anchor biosynthesis. ![Glycosylphosphatidylinositol(GPI)-anchor biosynthesis](./Mouse_RNA_Seq_p53_genotoxic_files/figure-html/mmu00563.pathview.png) 5.6.5 Pathway and regulation of genes for RNA polymerase. ![RNA polymerase](./Mouse_RNA_Seq_p53_genotoxic_files/figure-html/mmu03020.pathview.png) 5.6.6 Pathway and regulation of genes for Nucleotide excision repair. ![Nucleotide excision repair](./Mouse_RNA_Seq_p53_genotoxic_files/figure-html/mmu03420.pathview.png) 5.6.7 Pathway and regulation of genes for Non-homologous end-joining. ![Non-homologous end-joining](./Mouse_RNA_Seq_p53_genotoxic_files/figure-html/mmu03450.pathview.png) ##De novo Assembly {#denovo-assembly} AT THIS TIME, MINICONDA3 WAS INSTALLED SO FURTHER TOOLS ARE ALL INSTALLED UNDER THIS Having the genome for RNA-sequencing analysis is very useful, but sometimes it is not available so de novo assembly is used. de novo assembly is using the reads from sequencing to create longer seqeunces called contigs (contiguous sequences). These sequences are then compared to a protein database to get an idea of what proteins are possibly present. The raw sequences are also mapped back to the contigs, treating the contigs as the “genome,” much like we do when we have the genome from the host available. Lets jump right into it. We are going to be using Trinity for the de novo assembly and analysis because there are many tools built into the Trinity tool that allow for the building of contigs, counting of sequences, and even DeSeq/edgeR analyses. To install Trinity, all I did was ran conda install -c bioconda trinity. This installs all of the previously mentioned tools in one go. In addition to Trinity, BLAST (conda install -c bioconda blast) and RSEM (conda install -c bioconda rsem) were installed. Trinity --seqType fq \\ --left trimmedReads/SRR2121770_Trimmed_1P.fq.gz,trimmedReads/SRR2121771_Trimmed_1P.fq.gz,trimmedReads/SRR2121774_Trimmed_1P.fq.gz,trimmedReads/SRR2121775_Trimmed_1P.fq.gz,trimmedReads/SRR2121778_Trimmed_1P.fq.gz,trimmedReads/SRR2121779_Trimmed_1P.fq.gz,trimmedReads/SRR2121780_Trimmed_1P.fq.gz,trimmedReads/SRR2121781_Trimmed_1P.fq.gz,trimmedReads/SRR2121786_Trimmed_1P.fq.gz,trimmedReads/SRR2121787_Trimmed_1P.fq.gz,trimmedReads/SRR2121788_Trimmed_1P.fq.gz,trimmedReads/SRR2121789_Trimmed_1P.fq.gz \\ --right trimmedReads/SRR2121770_Trimmed_2P.fq.gz,trimmedReads/SRR2121771_Trimmed_2P.fq.gz,trimmedReads/SRR2121774_Trimmed_2P.fq.gz,trimmedReads/SRR2121775_Trimmed_2P.fq.gz,trimmedReads/SRR2121778_Trimmed_2P.fq.gz,trimmedReads/SRR2121779_Trimmed_2P.fq.gz,trimmedReads/SRR2121780_Trimmed_2P.fq.gz,trimmedReads/SRR2121781_Trimmed_2P.fq.gz,trimmedReads/SRR2121786_Trimmed_2P.fq.gz,trimmedReads/SRR2121787_Trimmed_2P.fq.gz,trimmedReads/SRR2121788_Trimmed_2P.fq.gz,trimmedReads/SRR2121789_Trimmed_2P.fq.gz \\ --CPU 80 --max_memory 2000G --min_contig_length 150 TrinityStats.pl trinity_out_dir/Trinity.fasta ################################ ## Counts of transcripts, etc. ################################ Total trinity &#39;genes&#39;: 602870 Total trinity transcripts: 730297 Percent GC: 45.89 ######################################## Stats based on ALL transcript contigs: ######################################## Contig N10: 8589 Contig N20: 5955 Contig N30: 4400 Contig N40: 3276 Contig N50: 2413 Median contig length: 413 Average contig: 1028.24 Total assembled bases: 750921120 ##################################################### ## Stats based on ONLY LONGEST ISOFORM per &#39;GENE&#39;: ##################################################### Contig N10: 5470 Contig N20: 3434 Contig N30: 2391 Contig N40: 1741 Contig N50: 1285 Median contig length: 343 Average contig: 703.04 Total assembled bases: 423841627 bowtie2-build trinity_out_dir/Trinity.fasta trinity_out_dir/Trinity.fasta bowtie2 --local --no-unal -x trinity_out ##download uniprot swiss-prot db ##ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz gunzip uniprot_sprot.fasta.gz mkdir -p blast_protdb mv uniprot_sprot.fasta blast_protdb/. makeblastdb -in uniprot_sprot.fasta -dbtype prot cd .. blastx -query trinity_out_dir/Trinity.fasta -db blast_protdb/uniprot_sprot.fasta -out blastx.outfmt6 -evalue 1e-20 -num_threads 80 -max_target_seqs 1 -outfmt 6 analyze_blastPlus_topHit_coverage.pl blastx.outfmt6 trinity_out_dir/Trinity.fasta blast_protdb/uniprot_sprot.fasta | column -t #hit_pct_cov_bin count_in_bin &gt;bin_below 100 7361 7361 90 1117 8478 80 981 9459 70 971 10430 60 1003 11433 50 865 12298 40 924 13222 30 1102 14324 20 1478 15802 10 932 16734 $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121770_1.fastq.gz -2 rawReads/SRR2121770_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121770.coordSorted.bam $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121771_1.fastq.gz -2 rawReads/SRR2121771_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121771.coordSorted.bam $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121774_1.fastq.gz -2 rawReads/SRR2121774_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121774.coordSorted.bam $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121775_1.fastq.gz -2 rawReads/SRR2121775_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121775.coordSorted.bam $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121778_1.fastq.gz -2 rawReads/SRR2121778_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121778.coordSorted.bam $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121779_1.fastq.gz -2 rawReads/SRR2121779_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121779.coordSorted.bam $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121780_1.fastq.gz -2 rawReads/SRR2121780_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121780.coordSorted.bam $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121781_1.fastq.gz -2 rawReads/SRR2121781_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121781.coordSorted.bam $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121786_1.fastq.gz -2 rawReads/SRR2121786_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121786.coordSorted.bam $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121787_1.fastq.gz -2 rawReads/SRR2121787_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121787.coordSorted.bam $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121788_1.fastq.gz -2 rawReads/SRR2121788_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121788.coordSorted.bam $ bowtie2 --local --no-unal -x trinity_out_dir/Trinity.fasta -q -1 rawReads/SRR2121789_1.fastq.gz -2 rawReads/SRR2121789_2.fastq.gz --threads 10 | samtools view --threads 10 -Sb - | samtools sort --threads 10 -o SRR2121789.coordSorted.bam $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121770_1.fastq.gz --right rawReads/SRR2121770_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121770.RSEM $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121771_1.fastq.gz --right rawReads/SRR2121771_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121771.RSEM $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121774_1.fastq.gz --right rawReads/SRR2121774_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121774.RSEM $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121775_1.fastq.gz --right rawReads/SRR2121775_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121775.RSEM $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121778_1.fastq.gz --right rawReads/SRR2121778_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121778.RSEM $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121779_1.fastq.gz --right rawReads/SRR2121779_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121779.RSEM $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121780_1.fastq.gz --right rawReads/SRR2121780_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121780.RSEM $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121781_1.fastq.gz --right rawReads/SRR2121781_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121781.RSEM $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121786_1.fastq.gz --right rawReads/SRR2121786_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121786.RSEM $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121787_1.fastq.gz --right rawReads/SRR2121787_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121787.RSEM $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121788_1.fastq.gz --right rawReads/SRR2121788_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121788.RSEM $ time align_and_estimate_abundance.pl --seqType fq --thread_count $SLURM_NTASKS --left rawReads/SRR2121789_1.fastq.gz --right rawReads/SRR2121789_2.fastq.gz --transcripts trinity_out_dir/Trinity.fasta --est_method RSEM --aln_method bowtie2 --trinity_mode --prep_reference --output_dir SRR2121789.RSEM time abundance_estimates_to_matrix.pl --est_method RSEM --out_prefix Trinity_trans SRR2121770.RSEM/SRR2121770.RSEM.trans.results \\ SRR2121771.RSEM/SRR2121771.RSEM.trans.results \\ SRR2121774.RSEM/SRR2121774.RSEM.trans.results \\ SRR2121775.RSEM/SRR2121775.RSEM.trans.results \\ SRR2121778.RSEM/SRR2121778.RSEM.trans.results \\ SRR2121779.RSEM/SRR2121779.RSEM.trans.results \\ SRR2121780.RSEM/SRR2121780.RSEM.trans.results \\ SRR2121781.RSEM/SRR2121781.RSEM.trans.results \\ SRR2121786.RSEM/SRR2121786.RSEM.trans.results \\ SRR2121787.RSEM/SRR2121787.RSEM.trans.results \\ SRR2121788.RSEM/SRR2121788.RSEM.trans.results \\ SRR2121789.RSEM/SRR2121789.RSEM.trans.results --gene_trans_map none time abundance_estimates_to_matrix.pl --est_method RSEM --out_prefix Trinity_genes SRR2121770.RSEM/SRR2121770.RSEM.genes.results \\ SRR2121771.RSEM/SRR2121771.RSEM.genes.results \\ SRR2121774.RSEM/SRR2121774.RSEM.genes.results \\ SRR2121775.RSEM/SRR2121775.RSEM.genes.results \\ SRR2121778.RSEM/SRR2121778.RSEM.genes.results \\ SRR2121779.RSEM/SRR2121779.RSEM.genes.results \\ SRR2121780.RSEM/SRR2121780.RSEM.genes.results \\ SRR2121781.RSEM/SRR2121781.RSEM.genes.results \\ SRR2121786.RSEM/SRR2121786.RSEM.genes.results \\ SRR2121787.RSEM/SRR2121787.RSEM.genes.results \\ SRR2121788.RSEM/SRR2121788.RSEM.genes.results \\ SRR2121789.RSEM/SRR2121789.RSEM.genes.results --gene_trans_map none contig_Exn50_statistic.pl Trinity_trans.TMM.EXPR.matrix trinity_out_dir/Trinity.fasta &gt; ExN50_trans.stats Make samples.txt mock- SRR2121770.RSEM mock- SRR2121771.RSEM IR- SRR2121774.RSEM IR- SRR2121775.RSEM mock+ SRR2121778.RSEM mock+ SRR2121779.RSEM mock+ SRR2121780.RSEM mock+ SRR2121781.RSEM IR+ SRR2121786.RSEM IR+ SRR2121787.RSEM IR+ SRR2121788.RSEM IR+ SRR2121789.RSEM The way that the installed version of run_DE_analysis.pl tries to install or check for edgeR doesn’t work for the new versions of R (3.6.1). In the perl script I just had to edit a few lines under the edgeR section for it to run smoothly, as well as install *BiocManager before hand. I changed the perl script, under sub run_edgeR_sample_pair, when writing the R script to look like: print $ofh &quot;if (! require(edgeR)) {\\n&quot;; print $ofh &quot; install.packages(\\&quot;BiocManager\\&quot;)\\n&quot;; print $ofh &quot; BiocManager::install(\\&quot;edgeR\\&quot;)\\n&quot;; print $ofh &quot; library(edgeR)\\n&quot;; print $ofh &quot;}\\n\\n&quot;; To make sure BiocManager and edgeR work, they were installed in the R terminal. R #runs the installed version of R in terminal mode install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;edgeR&quot;) Since we don’t have at least 3 replicates for each of the treatments, edgeR wants a dispersion parameter. 0.035 seems to be pretty common. If the dispersion parameter has the squareroot taken of it we get ~ 0.19, meaning that we are saying the true abundance for each gene can vary up or down by 19% between replicates. run_DE_analysis.pl --matrix Trinity_trans.counts.matrix --method edgeR --output edgeR_trans --dispersion 0.035 --samples_file samples.txt head edgeR_trans/Trinity_trans.counts.matrix.IR-_vs_mock-.edgeR.DE_results | column -t sampleA sampleB logFC logCPM PValue FDR TRINITY_DN2468_c0_g4_i1 IR- mock- -14.7753615825754 4.45315041918 5.51021995613131e-115 5.85185359341145e-110 TRINITY_DN2505_c0_g1_i2 IR- mock- -10.1817621187735 4.11815268306374 8.54695600746168e-88 4.53843363996215e-83 TRINITY_DN3231_c0_g2_i15 IR- mock- -13.5922145319911 3.27307743751541 6.29068575237937e-81 2.2269027563423e-76 TRINITY_DN4975_c0_g1_i7 IR- mock- 13.4941229058571 3.17132827495398 5.25978730838885e-79 1.39647353037724e-74 TRINITY_DN4360_c0_g1_i5 IR- mock- -13.5215746751431 3.20272600532575 1.0222845072488e-70 2.17133229339645e-66 TRINITY_DN128_c0_g1_i20 IR- mock- 13.1582688371709 2.83599676163376 1.8443766792831e-69 3.26454672233108e-65 TRINITY_DN644_c1_g1_i6 IR- mock- -5.91850038396836 4.31456225100214 8.01440426808572e-67 1.21589961895815e-62 TRINITY_DN2043_c0_g1_i13 IR- mock- 13.7803059024088 3.45719012794474 3.30519380237759e-66 4.38764477265625e-62 TRINITY_DN3321_c0_g1_i2 IR- mock- 12.8215942182179 2.49995284585527 9.5059376972195e-61 1.1217006482719e-56 #BiocManager::install(&quot;EnsDb.Mmusculus.v79&quot;) library(GO.db) library(GOstats) library(EnsDb.Mmusculus.v79) res.3 = subset(res.2, !is.na(res.2$entrezgene_id)) selectedGenes = unique(resSig[resSig$log2FoldChange&gt;0,&#39;entrezgene_id&#39;]) # upregulated genes universeGenes = unique( mapIds(org.Mm.eg.db, keys=gsub(&quot;\\\\.[0-9]*$&quot;,&quot;&quot;,res.3$entrezgene_id), # this is causing problems for mapping column=&quot;ENSEMBL&quot;, keytype=&quot;SYMBOL&quot;, multiVals=&quot;first&quot;) ) hgCutoff &lt;- 0.001 params &lt;- new(&quot;GOHyperGParams&quot;, geneIds=selectedGenes, universeGeneIds=universeGenes, annotation=&quot;org.Hs.eg.db&quot;, ontology=&quot;BP&quot;, pvalueCutoff=hgCutoff, conditional=FALSE, testDirection=&quot;over&quot;) hgOver &lt;- hyperGTest(params) summary(hgOver)[1:10,] #BiocManager::install(&quot;AnnotationHub&quot;) library(AnnotationHub) hub = AnnotationHub() query(hub, c(&quot;Medicago&quot;, &quot;OrgDb&quot;)) Mt.orgdb = hub[[&quot;AH72235&quot;]] Mt.orgdb #BiocManager::install(&quot;AnnotationHub&quot;) library(AnnotationHub) hub = AnnotationHub() query(hub, c(&quot;Glycine&quot;, &quot;OrgDb&quot;)) Gm.orgdb = hub[[&quot;AH72142&quot;]] Gm.orgdb library(AnnotationDbi) keytypes(Mt.orgdb) #key.Gm = keys(Mt.orgdb, keytype=&quot;GENENAME&quot;) #head(key.Gm) "],["wgcna-rna-seq-blog.html", "Chapter 6 WGCNA RNA-seq Blog 6.1 Introduction to the WGCNA Rpackage 6.2 RNA-seq Analisys with Kaku’s Data sample.", " Chapter 6 WGCNA RNA-seq Blog 6.1 Introduction to the WGCNA Rpackage 6.1.1 WGCNA Rpackage Installation procedure. Installing and re-installing new versions or R and Studio. In Terminal (or Rstudio &gt; Terminal). brew reinstall --cask r brew reinstall --cask rstudio Installing WGCNA: and R package for weighted correlation network analysis. To install the package, go to Tab: Packages &gt; Install …[]… -Repository: (CRAN); -Packages: WGCNA; *Install. In R code: install.packages(&quot;WGCNA&quot;) Installation (Failed). Unfortunately, it could not install correctly. Error was: install.packages(&quot;WGCNA&quot;) Warning in install.packages : dependencies ‘Biobase’, ‘impute’, ‘preprocessCore’, ‘GO.db’, ‘AnnotationDbi’ are not available Package which is only available in source form, and may need compilation of C/C++/Fortran: ‘WGCNA’ Do you want to attempt to install these from sources? (Yes/no/cancel) y Warning in install.packages : installation of package ‘WGCNA’ had non-zero exit status Dependencies are old and require special installation. From this page, installed several packages. (Failed). install.packages(c(&quot;dynamicTreeCut&quot;, &quot;cluster&quot;, &quot;flashClust&quot;, &quot;Hmisc&quot;, &quot;reshape&quot;, &quot;foreach&quot;, &quot;doParallel&quot;) ) source(&quot;http://bioconductor.org/biocLite.R&quot;) biocLite(&quot;impute&quot;) install.packages(&quot;WGCNA&quot;) Install most of the packages, but not “impute,” result: could not install WGCNA. Tried to install dependencies independently from here: (Failed) You&#39;re using R-3.6.0, but trying to install Bioconductor version 3.8. Try using BiocManager::install(&quot;Biobase&quot;). If that works, then try BiocManager::install(&quot;GO.db&quot;). If you have success, make sure your installation is valid with BiocManager::valid(). BiocManager::install(&quot;Biobase&quot;) BiocManager::install(&quot;GO.db&quot;) BiocManager::valid() No resutl Decided to install the complete “BiocManager” pack. ** BiocManager: (OK); (WGCNA:(Failed)** &gt; install.packages(&quot;BiocManager&quot;) downloaded 315 KB &gt; install.packages(&quot;WGCNA&quot;) Warning in install.packages : dependencies ‘impute’, ‘preprocessCore’ are not available Search ‘preprocessCore,’ in this page. PreprocessCore:(OK); (WGCNA:(Failed) To install this package, start R (version “4.1”) and enter: if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;preprocessCore&quot;) For ‘impute,’ found install in this page. Impute:(OK); WGCNA:( OK) To install this package, start R (version “4.1”) enter: if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;impute&quot;) Then install WGCNA install.packages(&quot;WGCNA&quot;) 6.1.2 Final installation workflow: # Set working directory getwd() setwd(&quot;/Users/marcelorosales/Box Sync/Documents/R/Rmarkdown&quot;) getwd() # Load list of packages and install. load(&quot;Rpackages&quot;) for (p in setdiff(packages, installed.packages()[,&quot;Package&quot;])) install.packages(p) Check &lt;- installed.packages() BiocManager::install(&quot;Biobase&quot;) BiocManager::install(&quot;GO.db&quot;) BiocManager::valid() &gt; install.packages(&quot;BiocManager&quot;) if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;preprocessCore&quot;) BiocManager::install(&quot;WGCNA&quot;) install.packages(c(&quot;dynamicTreeCut&quot;, &quot;cluster&quot;, &quot;flashClust&quot;, &quot;Hmisc&quot;, &quot;reshape&quot;, &quot;foreach&quot;, &quot;doParallel&quot;) ) source(&quot;http://bioconductor.org/biocLite.R&quot;) biocLite(&quot;impute&quot;) if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;impute&quot;) install.packages(&quot;WGCNA&quot;) Checking the packages for WGCNA installed previously for OS Catalina. Warning in install.packages : package ‘anRichment’ is not available for this version of R A version of this package for your version of R might be available elsewhere, see the ideas at https://cran.r-project.org/doc/manuals/r-patched/R-admin.htmlInstalling-packages Warning in install.packages : package ‘anRichment’ is not available for this version of R package ‘anRichmentMethods’ is not available for this version of R package ‘BiocFileCache’ is not available for this version of R package ‘BiocParallel’ is not available for this version of R package ‘biomaRt’ is not available for this version of R package ‘DelayedArray’ is not available for this version of R package ‘GenomicAlignments’ is not available for this version of R package ‘GenomicFeatures’ is not available for this version of R package ‘GenomicRanges’ is not available for this version of R package ‘inserttable’ is not available for this version of R package ‘lorem’ is not available for this version of R package ‘org.Hs.eg.db’ is not available for this version of R package ‘org.Mm.eg.db’ is not available for this version of R package ‘Rhtslib’ is not available for this version of R package ‘Rsamtools’ is not available for this version of R package ‘rtracklayer’ is not available for this version of R package ‘SummarizedExperiment’ is not available for this version of R package ‘TxDb.Hsapiens.UCSC.hg19.knownGene’ is not available for this version of R package ‘TxDb.Mmusculus.UCSC.mm10.knownGene’ is not available for this version of R A version of this package for your version of R might be available elsewhere, see the ideas at https://cran.r-project.org/doc/manuals/r-patched/R-admin.htmlInstalling-packages 6.1.3 WGCNA tutorial. This code has been adapted from the tutorials available at WGCNA website (this page does no longer exist). WGCNA: an R package for weighted correlation network analysis site. Tutorials for the WGCNA package. R tutorial Steps Required for this process are: 1. Data input and cleaning: PDF document, R script. 1. Network construction and module detection a. Automatic, one-step network construction and module detection: PDF document, R script b. Step-by-step network construction and module detection: PDF document, R script c. Dealing with large datasets: block-wise network construction and module detection: PDF document, R script 1. Relating modules to external clinical traits and identifying important genes: PDF document, R script 1. Interfacing network analysis with other data such as functional annotation and gene ontology PDF document, R script 1. Network visualization using WGCNA functions: PDF document, R script 1. Export of networks to external software: PDF document, R script 6.1.4 3.4 Gene expression analysis The level of gene expression is measured by read density, the higher the read density, the higher the level of gene expression. Gene expression calculation was performed with the formula below, which calculates FPKM (Fragments per kilo bases per million reads) based on read counts from HT-seq (V 0.6.1) (Mortazavi, 2008). The formula is: Figure 3.4.1 The ratio of (total exon fragments / mapped reads [millions]) is the read count mapped to the gene normalized to total read counts. The value is then normalized to gene length (exon length [KB]), so that the expression of genes with different sequencing depths and length are comparable. The numbers of genes with different expression levels are summarized in Table 3.7.1. In general, FPKM threshold for gene expression is set between 0.1-1, although there is no absolute standard and various thresholds have been used in the literature. 6.2 RNA-seq Analisys with Kaku’s Data sample. 2021/07/02 Copy Trial data from MK/土橋 to folder : The Report file. The Summary file Report ### Experimental Workflow. Transcriptome sequencing experiments include: * RNA extraction and QC (Quality Control?), * Library construction, * Purification, * Library QC and * Quantitation, as well as * Sequencing cluster generation and high through-put sequencing. Each step is important for data quality and quantity, which in turn affect the data analysis. To ensure the accuracy and reliability of the analysis results, every step is under strict monitoring and quality control. After mixing libraries based on their effective concentration and the required sequencing data volume, Illumina platform is used for high through-put sequencing. 6.2.1 Data Analysis. First Data must be in the right format. In the data received. “GeneName” “Chr” “Start” “End” “Strand” “Length” “mPDL_RNA14D_Ko1” “mPDL_RNA14D_Ko2” “mPDL_RNA14D_Ko3” “mPDL_RNA14D_Ko4” “mPDL_RNA14D_WT1” “mPDL_RNA14D_WT2” “mPDL_RNA14D_WT3” “mPDL_RNA14D_WT4” “mPDL_RNA7D_Ko1” “mPDL_RNA7D_Ko2” “mPDL_RNA7D_Ko3” “mPDL_RNA7D_Ko4” “mPDL_RNA7D_WT1” “mPDL_RNA7D_WT2” “mPDL_RNA7D_WT3” “mPDL_RNA7D_WT4” Use Markdown Table generator. 20210705 Problems with software* ** Regarding Rstudio Shortcut keys “not working” sometimes** SOME shortcuts will work if not on the corresponding file type. For example, MARKDOWN shortcuts will not work on R script files window or vice versa. Set working directory getwd() setwd(&quot;/Users/marcelorosales/Box Sync/Documents/R/Rmarkdown&quot;) &quot;/Users/marcelorosales/Box Sync/Documents/R/Rmarkdown&quot; # [1] &quot;/Users/marcelorosales/Box Sync/Niigata Uni Box/Experiments/Photoconvertible FP/Experiment Notebooks&quot; getwd() Load list of packages and install. load(&quot;Rpackages&quot;) for (p in setdiff(packages, installed.packages()[,&quot;Package&quot;])) install.packages(p) Check &lt;- installed.packages() BiocManager::install(&quot;Biobase&quot;) BiocManager::install(&quot;GO.db&quot;) BiocManager::valid() &gt; install.packages(&quot;BiocManager&quot;) if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;preprocessCore&quot;) BiocManager::install(&quot;WGCNA&quot;) install.packages(c(&quot;dynamicTreeCut&quot;, &quot;cluster&quot;, &quot;flashClust&quot;, &quot;Hmisc&quot;, &quot;reshape&quot;, &quot;foreach&quot;, &quot;doParallel&quot;) ) source(&quot;http://bioconductor.org/biocLite.R&quot;) biocLite(&quot;impute&quot;) install.packages(&quot;WGCNA&quot;) source(&quot;https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/GeneAnnotation/installAnRichment.R&quot;); installAnRichment(); install.packages(&quot;path/to/anRichmentMethods&quot;, repos = NULL, type = &quot;source&quot;); install.packages(&quot;path/to/anRichment&quot;, repos = NULL, type = &quot;source&quot;); source(&quot;https://bioconductor.org/biocLite.R&quot;); biocLite(c(&quot;AnnotationDBI&quot;, &quot;GO.db&quot;, &quot;org.Hs.eg.db&quot;, &quot;org.Mm.eg.db&quot;, &quot;XML&quot;, &quot;WGCNA&quot;, &quot;TxDb.Hsapiens.UCSC.hg19.knownGene&quot;, &quot;TxDb.Mmusculus.UCSC.mm10.knownGene&quot;)); install.packages(&quot;path/to/anRichmentMethods&quot;, repos = NULL, type = &quot;source&quot;); install.packages(&quot;path/to/anRichment&quot;, repos = NULL, type = &quot;source&quot;); BiocManager::install() source(&quot;https://bioconductor.org/biocLite.R&quot;); biocLite(c(&quot;AnnotationDBI&quot;, &quot;GO.db&quot;, &quot;org.Hs.eg.db&quot;, &quot;org.Mm.eg.db&quot;, &quot;XML&quot;, &quot;WGCNA&quot;, &quot;TxDb.Hsapiens.UCSC.hg19.knownGene&quot;, &quot;TxDb.Mmusculus.UCSC.mm10.knownGene&quot;)); if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;impute&quot;) str(all_fpkm) table(all_fpkm) names(all_fpkm) "],["youtube-references-.html", "Chapter 7 Youtube references. 7.1 Anaconda set up, Robust Data Science Environment with Miniconda and Conda-Forge. 7.2 Quality control &amp; preprocessing of raw reads 7.3 Trimmomatic. 7.4 Building Genome Index and Aligning with STAR", " Chapter 7 Youtube references. 7.1 Anaconda set up, Robust Data Science Environment with Miniconda and Conda-Forge. Anaconda set up Bioinformatics - Downloading and Setting Up Conda Environments. 7.2 Quality control &amp; preprocessing of raw reads Bioinformatics - SRA Download, QC, and Trimming. RNA-seq course: Quality control &amp; preprocessing of raw reads Good explanation of the fastQC html plots. 7.3 Trimmomatic. RNA Sequencing 3: Trimmomatic Good explanation of the fastQC html plots. Topic: Metagenomics Lesson 2 Demo 2.2 - Trimmomatic 7.4 Building Genome Index and Aligning with STAR Bioinformatics - Building Genome Index and Aligning with STAR How to use the Cat command "],["references.html", "References", " References "]]
